{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCXc2r4g3c",
        "outputId": "19a2d768-dcf0-42f5-bfec-33b6c8e9dda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Configuration\n",
        "create_dataset = False\n",
        "\n",
        "def in_google_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Get the platform\n",
        "if in_google_colab():\n",
        "    print(\"Running in Google Colab\")\n",
        "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
        "    !pip install vitaldb\n",
        "    directory = 'anesthesia_anomaly_detection/data/'\n",
        "    create_dataset = False\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    directory = 'data/'\n",
        "\n",
        "### Datasetpath\n",
        "datasetpath = 'dataset01/'\n",
        "vitaldbpath = 'vitaldb_tiva/'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import vitaldb as vf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "apfldbrf4g3g"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def outlierfilter(data: pd.DataFrame,threshhold: float, iterations: int, min: float, max: float):\n",
        "    \"\"\"\n",
        "    A filter function, which calculates the gradient of a given Pandas DataFram Timeseries\n",
        "    and performs a binary dilation on datapoints which exceed a certain treshhold, to detect\n",
        "    and remove unwanted outliers in the dataset. Additionally all values exceeding a given\n",
        "    min/max value are replaced with np.nan and linearly interpolated with the Pandas interpolate\n",
        "    method.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Timeseries Data\n",
        "        threshhold (float): Gradient thresshold\n",
        "        iterations (int): number of iterations of the binary dilation\n",
        "        min (float): maximum expected value\n",
        "        max (float): minimum expected value\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: _description_\n",
        "    \"\"\"\n",
        "    gradient = np.diff(data,n=1, axis=0, append=0)\n",
        "    gradientfilter = ndimage.binary_dilation(np.abs(gradient) > threshhold, iterations=iterations)\n",
        "\n",
        "    # Apply Filter\n",
        "    data[gradientfilter] = np.nan\n",
        "\n",
        "    data[data <= min] = np.nan\n",
        "    data[data > max] = np.nan\n",
        "\n",
        "    data = data.interpolate(method = 'linear')\n",
        "    data = data.bfill()\n",
        "    return data\n",
        "\n",
        "### Custom Normalization Functions\n",
        "\n",
        "def NormStandard(dataset: np.array):\n",
        "    mean = np.nanmean(dataset)\n",
        "    std = np.nanstd(dataset)\n",
        "    return (dataset - mean) / std\n",
        "\n",
        "def NormMinMax(dataset: np.array):\n",
        "    min = np.min(dataset)\n",
        "    max = np.max(dataset)\n",
        "    return (dataset - min) / (max - min)\n",
        "\n",
        "def NormCustomBIS(dataset: np.array):\n",
        "    return (100 - dataset) / 100\n",
        "\n",
        "def NormNone(dataset: np.array):\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GDGlya8F4g3h"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DatasetImport():\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str, interval: int = 10):\n",
        "        self.directory = directory\n",
        "        self.datasetpath = directory + dataset\n",
        "        self.vitalpath = directory + vitalpath\n",
        "\n",
        "        self.interval = interval\n",
        "\n",
        "        self.dataset = None\n",
        "        self.validation_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "        self.index = pd.read_csv(self.datasetpath +'dataset.csv', index_col=0).index.to_numpy()\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        np.savez_compressed(self.datasetpath+filename,\n",
        "                            train = self.train_dataset,\n",
        "                            validation = self.validation_dataset,\n",
        "                            test = self.test_dataset,\n",
        "                            timesteps = self.timesteps,\n",
        "                            )\n",
        "\n",
        "    def load(self, filename: str):\n",
        "        data = np.load(self.datasetpath+filename)\n",
        "        self.train_dataset = data['train']\n",
        "        self.validation_dataset = data['validation']\n",
        "        self.test_dataset = data['test']\n",
        "        try:\n",
        "            self.timesteps = data['timesteps']\n",
        "        except:\n",
        "            self.timesteps = []\n",
        "\n",
        "    def split(self,data):\n",
        "       train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
        "       train, validation = train_test_split(train, test_size=0.15, random_state=42)\n",
        "       return train, validation, test\n",
        "\n",
        "    def generateDataset(self, normalization):\n",
        "\n",
        "        dataset, self.timesteps = self.generate(self.index, normalization)\n",
        "\n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset = self.split(dataset)\n",
        "        print('Dataset succesfully generated                 ')\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "        batch_list = []\n",
        "        timesteps = []\n",
        "\n",
        "        for i, caseid in enumerate(dataset_index):\n",
        "            filepath = self.vitalpath+str(caseid).zfill(4)+'.vital'\n",
        "            data, importName = self.importFunction(filepath)\n",
        "            timesteps.append(data.shape[0])\n",
        "            batch_list.append(data)\n",
        "            print(importName + \" Fortschritt: %.1f\" % (100 * (i+1) / len(dataset_index)),' % ', end='\\r')\n",
        "\n",
        "        ### Pad the dataset\n",
        "        data = tf.keras.preprocessing.sequence.pad_sequences(batch_list, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "        # Remove 0.0 padded values\n",
        "        data[data == 0.0] = np.nan\n",
        "\n",
        "        # Nomalization\n",
        "        data = normalization(data)\n",
        "\n",
        "        # restore padded values\n",
        "        np.nan_to_num(data, copy=False, nan=0.0)\n",
        "\n",
        "        return data, np.array(timesteps)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        return None, None\n",
        "\n",
        "class infoImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.columns = ['sex','age','height','weight','bmi']\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "\n",
        "        data = pd.read_csv(self.directory+'info_vitaldb/cases.csv', index_col=0)\n",
        "        data = data[self.columns].loc[dataset_index].to_numpy()\n",
        "\n",
        "        sex = np.where(data[:, 0] == 'F', -0.5, 0.5)\n",
        "\n",
        "        data = data[:,1:].astype(float)\n",
        "        data = np.c_[sex, normalization(data)]\n",
        "\n",
        "        return data, None\n",
        "\n",
        "class VitalImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.tracks = []\n",
        "        self.filter = [0,0,0]\n",
        "        self.name = 'Vital'\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "\n",
        "        vitaldata = vf.VitalFile(ipath = filepath, track_names = self.tracks)\n",
        "\n",
        "        data = vitaldata.to_pandas(track_names=self.tracks,interval=self.interval)\n",
        "        data = data + 0.00001 # adds small value to avoid mix up with padding values\n",
        "        data = outlierfilter(data, threshhold = self.filter[0] , iterations = 2, min = self.filter[1], max = self.filter[2])\n",
        "\n",
        "        return data, self.name\n",
        "\n",
        "class BPImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        pressureWave = vf.VitalFile(filepath).to_numpy(['SNUADC/ART'], 1/500)\n",
        "\n",
        "        samples = self.interval * 500\n",
        "\n",
        "        # Remove values which derivative is too large\n",
        "        gradient = np.diff(pressureWave,n=1, axis=0, append=0)\n",
        "        gradientfilter1 = ndimage.binary_dilation(np.abs(gradient) > 4,iterations=30)\n",
        "        gradientfilter2 = ndimage.binary_dilation(np.abs(gradient) > 7,iterations=1000)\n",
        "        pressureWave[gradientfilter1] = np.nan\n",
        "        pressureWave[gradientfilter2] = np.nan\n",
        "\n",
        "        # Remove the negative values and values above 250\n",
        "        pressureWave[pressureWave <= 20] = np.nan\n",
        "        pressureWave[pressureWave > 250] = np.nan\n",
        "\n",
        "        pressureWave = self.imputer1.fit_transform(pressureWave)\n",
        "\n",
        "        ### Reshape the pressureWave to 1000 samples (2 seconds) per row\n",
        "        #if (pressureWave.shape[0] % samples) != 0 :\n",
        "        #    steps2fill = samples - (pressureWave.shape[0] % samples)\n",
        "        #    pressureWave = np.pad(array=pressureWave, pad_width=((0,steps2fill),(0,0)), mode='constant', constant_values=np.nan)\n",
        "        length = pressureWave.shape[0] - (pressureWave.shape[0] % samples)\n",
        "        pressureWave = pressureWave[0:length]\n",
        "        return pressureWave.reshape(-1,samples), 'Blood Pressure'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7QPUKpAQ4g3i"
      },
      "outputs": [],
      "source": [
        "###### Create Dataset\n",
        "if create_dataset:\n",
        "    bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bis.name = 'Bispektralindex'\n",
        "    bis.tracks = ['BIS/BIS']\n",
        "    bis.filter = [20, 10, 100]\n",
        "    bis.generateDataset(normalization=NormCustomBIS)\n",
        "    bis.save('00_bis.npz')\n",
        "\n",
        "    info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    info.generateDataset(normalization=NormStandard)\n",
        "    info.save('01_info.npz')\n",
        "\n",
        "    bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bloodpressure.name = 'bloodpressure'\n",
        "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
        "    bloodpressure.filter = [20, 20, 250]\n",
        "    bloodpressure.generateDataset(normalization=NormStandard)\n",
        "    bloodpressure.save('02_bloodpressure.npz')\n",
        "\n",
        "    etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    etCO2.name = 'End Tidal CO2'\n",
        "    etCO2.tracks = ['Primus/ETCO2']\n",
        "    etCO2.filter = [5, 15, 50]\n",
        "    etCO2.generateDataset(normalization=NormStandard)\n",
        "    etCO2.save('02_etCO2.npz')\n",
        "\n",
        "    spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    spO2.name = 'SpO2'\n",
        "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
        "    spO2.filter = [3, 80, 100]\n",
        "    spO2.generateDataset(normalization=NormStandard)\n",
        "    spO2.save('02_spO2.npz')\n",
        "\n",
        "    hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    hr.name = 'Heart Rate'\n",
        "    hr.tracks = ['Solar8000/HR']\n",
        "    hr.filter = [20, 40, 180]\n",
        "    hr.generateDataset(normalization=NormStandard)\n",
        "    hr.save('02_hr.npz')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xebtgcoK4g3i"
      },
      "outputs": [],
      "source": [
        "### Load the datasets\n",
        "bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bis.load('00_bis.npz')\n",
        "\n",
        "info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "info.load('01_info.npz')\n",
        "\n",
        "bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bloodpressure.load('02_bloodpressure.npz')\n",
        "\n",
        "etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "etCO2.load('02_etCO2.npz')\n",
        "\n",
        "spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "spO2.load('02_spO2.npz')\n",
        "\n",
        "hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "hr.load('02_hr.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auUnKLkV4g3j",
        "outputId": "c6eabb32-e54f-41ad-ea24-a969b34be8e1"
      },
      "outputs": [],
      "source": [
        "########################################## COMBINED MODEL ##########################################\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
        "\n",
        "### Combine the vital data\n",
        "vital_train = np.concatenate([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, hr.train_dataset], axis=2)\n",
        "vital_validation = np.concatenate([bloodpressure.validation_dataset, etCO2.validation_dataset, spO2.validation_dataset, hr.validation_dataset], axis=2)\n",
        "vital_test = np.concatenate([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, hr.test_dataset], axis=2)\n",
        "\n",
        "y_train = pd.DataFrame(bis.train_dataset[:,:,0].T).rolling(min_periods=1,window=30, center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
        "\n",
        "\n",
        "def simple_model(x_train, y_train, x_val, y_val, params):\n",
        "    ### LSTM layers for the vital data\n",
        "    input_vital = Input(shape=(None, vital_train.shape[2]))\n",
        "    vital_layer = Masking(mask_value=0.0)(input_vital)\n",
        "\n",
        "    ### INFO layers\n",
        "    input_info = Input(shape=(info.train_dataset.shape[1],))\n",
        "    info_layer = RepeatVector(vital_train.shape[1])(input_info)\n",
        "\n",
        "    ## Concatenate the LSTM output with the info layer\n",
        "    comb_layer = Concatenate()([vital_layer, info_layer])\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    #comb_layer = BatchNormalization()(comb_layer)\n",
        "    #comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    #comb_layer = BatchNormalization()(comb_layer)\n",
        "    #comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    #comb_layer = BatchNormalization()(comb_layer)\n",
        "    #comb_layer = Dense(units=128, activation='relu')(comb_layer)\n",
        "    #comb_layer = BatchNormalization()(comb_layer)\n",
        "    #comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
        "    #comb_layer = BatchNormalization()(comb_layer)\n",
        "\n",
        "\n",
        "    output = Dense(units=1)(comb_layer)\n",
        "    output = ReLU(max_value=1.0)(output)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[input_vital, input_info], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                #loss=tf.keras.losses.Huber(),\n",
        "                loss=tf.keras.losses.MeanSquaredError(),\n",
        "                metrics=['MeanSquaredError','MeanAbsoluteError']\n",
        "                )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    out = model.fit(x_train,\n",
        "                        y_train,\n",
        "                        validation_data=[x_val, y_val],\n",
        "                        epochs=30,\n",
        "                        batch_size=params['batch_size']\n",
        "                        )\n",
        "    return out, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ any_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3680</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ expand_dims_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ any_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ExpandDims</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ broadcast_to_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ expand_dims_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BroadcastTo</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ones_like_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3680</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ repeat_vector_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OnesLike</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masking_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ broadcast_to_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ ones_like_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ masking_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ repeat_vector_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ any_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,632</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │                   │            │ any_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ any_2 (\u001b[38;5;33mAny\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ repeat_vector_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3680\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ expand_dims_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ any_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mExpandDims\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ broadcast_to_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ expand_dims_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBroadcastTo\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ones_like_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3680\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ repeat_vector_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mOnesLike\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masking_1 (\u001b[38;5;33mMasking\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ broadcast_to_1[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ ones_like_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ masking_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ repeat_vector_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ any_3 (\u001b[38;5;33mAny\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m5,632\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │                   │            │ any_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │         \u001b[38;5;34m33\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,665</span> (22.13 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,665\u001b[0m (22.13 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,665</span> (22.13 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,665\u001b[0m (22.13 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, None, 6) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(None, 3680, 1), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtalos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m p \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m],\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m scan_object \u001b[38;5;241m=\u001b[39m \u001b[43mtalos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvital_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvital_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                         \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimpleModel_BatchSize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmulti_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimple_model\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv-non-metal/lib/python3.11/site-packages/talos/scan/Scan.py:205\u001b[0m, in \u001b[0;36mScan.__init__\u001b[0;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, multi_input, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights, save_models)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# start runtime\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_run\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scan_run\n\u001b[0;32m--> 205\u001b[0m \u001b[43mscan_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv-non-metal/lib/python3.11/site-packages/talos/scan/scan_run.py:26\u001b[0m, in \u001b[0;36mscan_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# otherwise proceed with next permutation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_round\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scan_round\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mscan_round\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# close progress bar before finishing\u001b[39;00m\n",
            "File \u001b[0;32m~/venv-non-metal/lib/python3.11/site-packages/talos/scan/scan_round.py:19\u001b[0m, in \u001b[0;36mscan_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mingest_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ingest_model\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_history, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround_model \u001b[38;5;241m=\u001b[39m \u001b[43mingest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# handle logging of results\u001b[39;00m\n",
            "File \u001b[0;32m~/venv-non-metal/lib/python3.11/site-packages/talos/model/ingest_model.py:6\u001b[0m, in \u001b[0;36mingest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mingest_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Ingests the model that is input by the user\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    through Scan() model paramater.'''\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 57\u001b[0m, in \u001b[0;36msimple_model\u001b[0;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, model\n",
            "File \u001b[0;32m~/venv-non-metal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/venv-non-metal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, None, 6) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(None, 3680, 1), dtype=bool)"
          ]
        }
      ],
      "source": [
        "import talos\n",
        "\n",
        "p = {\n",
        "    'batch_size': [2, 4, 8, 16, 32],\n",
        "}\n",
        "\n",
        "scan_object = talos.Scan(x=[vital_train, info.train_dataset],\n",
        "                         y=y_train,\n",
        "                         x_val=[vital_validation, info.validation_dataset],\n",
        "                         y_val=bis.validation_dataset,\n",
        "                         experiment_name='SimpleModel_BatchSize',\n",
        "                         multi_input=True,\n",
        "                         params=p,\n",
        "                         model=simple_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "6J0t32kS4g3j",
        "outputId": "ce573a56-63a5-4a7d-97d0-c1f55a561ffb"
      },
      "outputs": [],
      "source": [
        "########################################## TRAINING SUMMARY ##########################################\n",
        "\n",
        "# Evaluate the mod\n",
        "train_score = history.history\n",
        "# Plot configuration\n",
        "plt.figure(figsize=(15/2.54, 8/2.54))\n",
        "\n",
        "# Actual plot\n",
        "plt.plot(train_score['loss'], label='Training Loss', color='k', marker='^', markerfacecolor='k')\n",
        "plt.plot(train_score['val_loss'], label='Validation Loss', color='k', marker='s', markerfacecolor='w')\n",
        "\n",
        "# Title and labels\n",
        "#plt.title('Training Model loss')\n",
        "plt.xlabel('Training Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlim(0, 30)\n",
        "plt.legend()\n",
        "\n",
        "# Axis settings\n",
        "ax = plt.gca()\n",
        "ax.spines['left'].set_linewidth(1.5)\n",
        "ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "\n",
        "\n",
        "plt.grid(True, linewidth=1.0)\n",
        "\n",
        "#plt.savefig('figure.eps', format='eps')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8tKp7Jsz4g3k",
        "outputId": "05662042-e9b6-4829-a06d-55c2feba36ea"
      },
      "outputs": [],
      "source": [
        "########################################## TESTING ##########################################\n",
        "import matplotlib.pyplot as plt\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Prediction\n",
        "y_test = []\n",
        "y_pred_test = []\n",
        "\n",
        "fig, axs = plt.subplots(bis.test_dataset.shape[0], 1, figsize=(10, 100))\n",
        "\n",
        "y_pred =  model.predict([vital_test, info.test_dataset])\n",
        "y_pred = - (y_pred * 100) + 100\n",
        "\n",
        "ground_truth = - (bis.test_dataset * 100) + 100\n",
        "\n",
        "test_index = bis.split(np.array(bis.index))[2]\n",
        "timesteps = bis.split(bis.timesteps)[2]\n",
        "\n",
        "### Evaluate Test Score\n",
        "test_score = model.evaluate([vital_test, info.test_dataset], bis.test_dataset, verbose=False)\n",
        "\n",
        "t = PrettyTable(['Metric', 'Train Score', 'Test Score'])\n",
        "#for i, metric in enumerate(model.metrics_names):\n",
        "#  t.add_row([metric, train_score[metric][-1], test_score[i]])\n",
        "#print(t)\n",
        "\n",
        "### Plot Test Results\n",
        "\n",
        "for j, filename in enumerate(test_index):\n",
        "    axs[j].plot(ground_truth[j,:,:], label='Ground Truth')\n",
        "    axs[j].plot(y_pred[j,:,:], label='Prediction')\n",
        "    axs[j].legend()\n",
        "    axs[j].set_title('Case ID: ' + str(filename))\n",
        "    axs[j].axis([0,timesteps[j],0,100])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
