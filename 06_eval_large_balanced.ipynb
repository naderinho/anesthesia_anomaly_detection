{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCXc2r4g3c",
        "outputId": "072c1724-5499-48cb-af6c-131e931b13b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Configuration\n",
        "create_dataset = True\n",
        "\n",
        "def in_google_colab():\n",
        "    \"\"\"Checks if the code is running in Google Colab\n",
        "\n",
        "    Returns:\n",
        "        bool: _description_\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if in_google_colab():\n",
        "    print(\"Running in Google Colab\")\n",
        "    # Install necessary packages in Google Colab\n",
        "    !rm -r sample_data/\n",
        "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
        "    !cp -r anesthesia_anomaly_detection/* .\n",
        "    !rm -r anesthesia_anomaly_detection/\n",
        "    !pip install vitaldb\n",
        "    create_dataset = False\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "\n",
        "### Datasetpath\n",
        "directory = 'data/'\n",
        "datasetpath = 'dataset03/'\n",
        "vitaldbpath = 'vitaldb_sevofl/'\n",
        "\n",
        "### Import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import vitaldb as vf\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "### Custom functions\n",
        "import modules as md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7QPUKpAQ4g3i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset succesfully generated                 \n",
            "Dataset succesfully generated                 \n",
            "Dataset succesfully generated                 \n",
            "Dataset succesfully generated                 \n",
            "Dataset succesfully generated                 \n",
            "Dataset succesfully generated                 \n",
            "Dataset succesfully generated                 \n"
          ]
        }
      ],
      "source": [
        "###### Create Dataset\n",
        "if create_dataset:\n",
        "    bis = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bis.name = 'Bispektralindex'\n",
        "    bis.tracks = ['BIS/BIS']\n",
        "    bis.filter = [20, 10, 100]\n",
        "    bis.generateDataset(normalization=md.NormNone)\n",
        "    bis.save('00_bis.npz')\n",
        "\n",
        "    info = md.infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    info.generateDataset(normalization=md.NormStandard)\n",
        "    info.save('01_info.npz')\n",
        "\n",
        "    bloodpressure = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bloodpressure.name = 'bloodpressure'\n",
        "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
        "    bloodpressure.filter = [20, 20, 250]\n",
        "    bloodpressure.generateDataset(normalization=md.NormStandard)\n",
        "    bloodpressure.save('02_bloodpressure.npz')\n",
        "\n",
        "    etCO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    etCO2.name = 'End Tidal CO2'\n",
        "    etCO2.tracks = ['Primus/ETCO2']\n",
        "    etCO2.filter = [5, 15, 50]\n",
        "    etCO2.generateDataset(normalization=md.NormStandard)\n",
        "    etCO2.save('02_etCO2.npz')\n",
        "\n",
        "    spO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    spO2.name = 'SpO2'\n",
        "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
        "    spO2.filter = [3, 80, 100]\n",
        "    spO2.generateDataset(normalization=md.NormStandard)\n",
        "    spO2.save('02_spO2.npz')\n",
        "\n",
        "    hr = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    hr.name = 'Heart Rate'\n",
        "    hr.tracks = ['Solar8000/HR']\n",
        "    hr.filter = [20, 40, 180]\n",
        "    hr.generateDataset(normalization=md.NormStandard)\n",
        "    hr.save('02_hr.npz')\n",
        "\n",
        "    mac = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    mac.name = 'MAC'\n",
        "    mac.tracks = ['Primus/MAC']\n",
        "    mac.filterON = False\n",
        "    mac.generateDataset(normalization=md.NormNone)\n",
        "    mac.save('03_mac.npz')\n",
        "\n",
        "### Load the datasets\n",
        "bis = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bis.load('00_bis.npz')\n",
        "\n",
        "info = md.infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "info.load('01_info.npz')\n",
        "\n",
        "bloodpressure = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bloodpressure.load('02_bloodpressure.npz')\n",
        "\n",
        "etCO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "etCO2.load('02_etCO2.npz')\n",
        "\n",
        "spO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "spO2.load('02_spO2.npz')\n",
        "\n",
        "hr = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "hr.load('02_hr.npz')\n",
        "\n",
        "mac = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "mac.load('03_mac.npz')\n",
        "\n",
        "\n",
        "train_index, val_index, test_index = bis.split(np.array(bis.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auUnKLkV4g3j",
        "outputId": "31e101db-a25d-46ea-a3ff-000922cf0468"
      },
      "outputs": [],
      "source": [
        "########################################## COMBINED MODEL ##########################################\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector, Lambda\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
        "\n",
        "### Blood Pressure Input\n",
        "input_bp = Input(shape=(None, bloodpressure.train_dataset.shape[2]))\n",
        "bp_layer = Masking(mask_value=0.0)(input_bp)\n",
        "\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=128, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=16, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=16, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = Dense(units=16, activation='linear')(bp_layer)\n",
        "\n",
        "### etCO2 Input\n",
        "input_co2 = Input(shape=(None, etCO2.train_dataset.shape[2]))\n",
        "\n",
        "co2_layer = LSTM(units=64, return_sequences=True)(input_co2)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=64, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=64, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=16, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=8, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = Dense(units=8, activation='linear')(co2_layer)\n",
        "\n",
        "### spo2 Input\n",
        "input_spo2 = Input(shape=(None, spO2.train_dataset.shape[2]))\n",
        "\n",
        "spo2_layer = LSTM(units=64, return_sequences=True)(input_spo2)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=64, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=64, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=16, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=8, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = Dense(units=8, activation='linear')(spo2_layer)\n",
        "\n",
        "# Sevoflurane layers\n",
        "input_mac = Input(shape=(None, mac.train_dataset.shape[2]))\n",
        "mac_layer = BatchNormalization()(input_mac)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=128, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=16, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=16, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = Dense(units=16, activation='linear')(mac_layer)\n",
        "\n",
        "### INFO layers\n",
        "input_info = Input(shape=(info.train_dataset.shape[1],))\n",
        "info_layer = RepeatVector(bloodpressure.train_dataset.shape[1])(input_info)\n",
        "info_layer = Dense(units=16, activation='linear')(info_layer)\n",
        "info_layer = Dense(units=16, activation='sigmoid')(info_layer)\n",
        "\n",
        "\n",
        "## Concatenate the Sevoflurane output with the info layer\n",
        "comb_layer = Concatenate()([bp_layer, co2_layer, spo2_layer, mac_layer, info_layer])\n",
        "comb_layer = Dense(units=64, activation='linear')(comb_layer)\n",
        "comb_layer = Dense(units=64, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=64, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=16, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=1, activation=ReLU(max_value=1.0))(comb_layer)\n",
        "output = Lambda(lambda x: x * 100)(comb_layer)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_bp, input_co2, input_spo2, input_mac, input_info], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['MeanSquaredError','MeanAbsoluteError','RootMeanSquaredError']\n",
        "              )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rolling mean on BIS data\n",
        "y = pd.DataFrame(bis.train_dataset[:,:,0].T).rolling(min_periods=1,window=3, center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
        "\n",
        "# Define training stop criteria\n",
        "class StopTrainingCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(StopTrainingCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_loss') is not None:\n",
        "            if logs.get('val_loss') < self.threshold:\n",
        "                print(f\"\\nValidation loss is below {self.threshold}, stopping training.\")\n",
        "                self.model.stop_training = True\n",
        "\n",
        "stop_training_callback = StopTrainingCallback(threshold = 35)\n",
        "\n",
        "\n",
        "if in_google_colab():\n",
        "    # Train the model\n",
        "    history = model.fit([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, mac.train_dataset, info.train_dataset],\n",
        "                        y,\n",
        "                        validation_data=([bloodpressure.validation_dataset, etCO2.validation_dataset, spO2.validation_dataset, mac.validation_dataset, info.validation_dataset], bis.validation_dataset),\n",
        "                        epochs=150,\n",
        "                        callbacks=[stop_training_callback],\n",
        "                        batch_size=4\n",
        "                        )\n",
        "\n",
        "    train_score = history.history\n",
        "\n",
        "    # Save the model\n",
        "    model.save('download/model.keras')\n",
        "\n",
        "    # Save the training history\n",
        "    with open('download/train_score.pkl', 'wb') as f:\n",
        "        pickle.dump(train_score, f)\n",
        "\n",
        "    # Save the prediction\n",
        "    y_pred = model.predict([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, mac.test_dataset, info.test_dataset], verbose=0)\n",
        "    with open('download/prediction.pkl', 'wb') as f:\n",
        "        pickle.dump(y_pred, f)\n",
        "\n",
        "else: \n",
        "    # Load train score data\n",
        "    with open('train_score.pkl', 'rb') as f:\n",
        "        train_score = pickle.load(f)\n",
        "    \n",
        "    # Load test prediction data\n",
        "    with open('prediction.pkl', 'rb') as f:\n",
        "        y_pred = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.plotting import training_loss_plot\n",
        "\n",
        "plot = training_loss_plot(train_score, filename='download/training_loss.pdf')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Predict on the test set\n",
        "from utils.evaluation import phases_report, phases_report_std\n",
        "\n",
        "print('Testmetriken:')\n",
        "\n",
        "report = phases_report(y_pred, bis.test_dataset, mac.test_dataset)\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "phases_report_std(report, y_pred, bis.test_dataset, mac.test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.plotting import full_histogramm_plot\n",
        "\n",
        "plot = full_histogramm_plot(groundtruth = bis.test_dataset, prediction = y_pred, filename='download/histogramm.pdf')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.plotting import single_prediction_plot\n",
        "\n",
        "for case in test_index:\n",
        "    single_prediction_plot(\n",
        "        case = case, \n",
        "        index = test_index, \n",
        "        groundtruth = bis.test_dataset, \n",
        "        prediction = y_pred, \n",
        "        infusion = mac.test_dataset, \n",
        "        error = 'Prediction RMSE',\n",
        "        filename = 'download/' + str(case) + '.pdf')\n",
        "\n",
        "print('Finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.plotting import full_prediction_plot\n",
        "\n",
        "full_prediction_plot(index = test_index, groundtruth = bis.test_dataset, prediction = y_pred, infusion = mac.test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
