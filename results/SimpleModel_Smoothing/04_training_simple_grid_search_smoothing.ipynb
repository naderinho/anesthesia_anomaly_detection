{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCXc2r4g3c",
        "outputId": "282b2f9e-0ec0-4702-801d-e13148dc2d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Cloning into 'anesthesia_anomaly_detection'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Counting objects: 100% (234/234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 234 (delta 89), reused 184 (delta 48), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (234/234), 40.43 MiB | 9.61 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Updating files: 100% (58/58), done.\n",
            "Collecting vitaldb\n",
            "  Downloading vitaldb-1.4.9-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astetik\n",
            "  Downloading astetik-1.16-py2.py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chances\n",
            "  Downloading chances-0.1.9-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kerasplotlib\n",
            "  Downloading kerasplotlib-1.0-py3-none-any.whl (4.3 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting wrangle\n",
            "  Downloading wrangle-0.7.6-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vitaldb) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.31.0)\n",
            "Collecting wfdb (from vitaldb)\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geonamescache (from astetik)\n",
            "  Downloading geonamescache-2.0.0-py3-none-any.whl (26.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from astetik) (7.34.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from astetik) (0.5.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from astetik) (1.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from astetik) (0.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from chances) (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from kerasplotlib) (3.7.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from wrangle) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->astetik) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->astetik)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->astetik) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kerasplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kerasplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kerasplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kerasplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kerasplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->kerasplotlib) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->astetik) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->astetik) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (3.20.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->wrangle) (2.15.0)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->wrangle) (0.43.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->astetik) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->astetik) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->astetik) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->wrangle) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->wrangle) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->wrangle) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->wrangle) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->wrangle) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb->vitaldb) (2.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->wrangle) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->wrangle) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->wrangle) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->wrangle) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->wrangle) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->wrangle) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->wrangle) (3.2.2)\n",
            "Installing collected packages: geonamescache, jedi, chances, wfdb, kerasplotlib, vitaldb, wrangle, astetik\n",
            "Successfully installed astetik-1.16 chances-0.1.9 geonamescache-2.0.0 jedi-0.19.1 kerasplotlib-1.0 vitaldb-1.4.9 wfdb-4.1.2 wrangle-0.7.6\n",
            "Collecting talos\n",
            "  Downloading talos-1.4-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: talos\n",
            "Successfully installed talos-1.4\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Configuration\n",
        "create_dataset = False\n",
        "\n",
        "def in_google_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Get the platform\n",
        "if in_google_colab():\n",
        "    print(\"Running in Google Colab\")\n",
        "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
        "    !pip install vitaldb astetik chances kerasplotlib statsmodels tqdm wrangle\n",
        "    !pip install --no-deps talos\n",
        "    directory = 'anesthesia_anomaly_detection/data/'\n",
        "    create_dataset = False\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    directory = 'data/'\n",
        "\n",
        "### Datasetpath\n",
        "datasetpath = 'dataset02/'\n",
        "vitaldbpath = 'vitaldb_tiva/'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import vitaldb as vf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "apfldbrf4g3g"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def outlierfilter(data: pd.DataFrame,threshhold: float, iterations: int, min: float, max: float):\n",
        "    \"\"\"\n",
        "    A filter function, which calculates the gradient of a given Pandas DataFram Timeseries\n",
        "    and performs a binary dilation on datapoints which exceed a certain treshhold, to detect\n",
        "    and remove unwanted outliers in the dataset. Additionally all values exceeding a given\n",
        "    min/max value are replaced with np.nan and linearly interpolated with the Pandas interpolate\n",
        "    method.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Timeseries Data\n",
        "        threshhold (float): Gradient thresshold\n",
        "        iterations (int): number of iterations of the binary dilation\n",
        "        min (float): maximum expected value\n",
        "        max (float): minimum expected value\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: _description_\n",
        "    \"\"\"\n",
        "    gradient = np.diff(data,n=1, axis=0, append=0)\n",
        "    gradientfilter = ndimage.binary_dilation(np.abs(gradient) > threshhold, iterations=iterations)\n",
        "\n",
        "    # Apply Filter\n",
        "    data[gradientfilter] = np.nan\n",
        "\n",
        "    data[data <= min] = np.nan\n",
        "    data[data > max] = np.nan\n",
        "\n",
        "    data = data.interpolate(method = 'linear')\n",
        "    data = data.bfill()\n",
        "    return data\n",
        "\n",
        "### Custom Normalization Functions\n",
        "\n",
        "def NormStandard(dataset: np.array):\n",
        "    mean = np.nanmean(dataset)\n",
        "    std = np.nanstd(dataset)\n",
        "    return (dataset - mean) / std\n",
        "\n",
        "def NormMinMax(dataset: np.array):\n",
        "    min = np.min(dataset)\n",
        "    max = np.max(dataset)\n",
        "    return (dataset - min) / (max - min)\n",
        "\n",
        "def NormCustomBIS(dataset: np.array):\n",
        "    return (100 - dataset) / 100\n",
        "\n",
        "def NormNone(dataset: np.array):\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GDGlya8F4g3h"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DatasetImport():\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str, interval: int = 10):\n",
        "        self.directory = directory\n",
        "        self.datasetpath = directory + dataset\n",
        "        self.vitalpath = directory + vitalpath\n",
        "\n",
        "        self.interval = interval\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "        self.index = pd.read_csv(self.datasetpath +'dataset.csv', index_col=0).index.to_numpy()\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        np.savez_compressed(self.datasetpath+filename,\n",
        "                            train = self.train_dataset,\n",
        "                            validation = self.validation_dataset,\n",
        "                            test = self.test_dataset,\n",
        "                            timesteps = self.timesteps,\n",
        "                            )\n",
        "\n",
        "    def load(self, filename: str):\n",
        "        data = np.load(self.datasetpath+filename)\n",
        "        self.train_dataset = data['train']\n",
        "        self.validation_dataset = data['validation']\n",
        "        self.test_dataset = data['test']\n",
        "        try:\n",
        "            self.timesteps = data['timesteps']\n",
        "        except:\n",
        "            self.timesteps = []\n",
        "\n",
        "    def split(self,data):\n",
        "       train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
        "       train, validation = train_test_split(train, test_size=0.15, random_state=42)\n",
        "       return train, validation, test\n",
        "\n",
        "    def generateDataset(self, normalization):\n",
        "\n",
        "        dataset, self.timesteps = self.generate(self.index, normalization)\n",
        "\n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset = self.split(dataset)\n",
        "        print('Dataset succesfully generated                 ')\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "        batch_list = []\n",
        "        timesteps = []\n",
        "\n",
        "        for i, caseid in enumerate(dataset_index):\n",
        "            filepath = self.vitalpath+str(caseid).zfill(4)+'.vital'\n",
        "            data, importName = self.importFunction(filepath)\n",
        "            timesteps.append(data.shape[0])\n",
        "            batch_list.append(data)\n",
        "            print(importName + \" Fortschritt: %.1f\" % (100 * (i+1) / len(dataset_index)),' % ', end='\\r')\n",
        "\n",
        "        ### Pad the dataset\n",
        "        data = tf.keras.preprocessing.sequence.pad_sequences(batch_list, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "        # Remove 0.0 padded values\n",
        "        data[data == 0.0] = np.nan\n",
        "\n",
        "        # Nomalization\n",
        "        data = normalization(data)\n",
        "\n",
        "        # restore padded values\n",
        "        np.nan_to_num(data, copy=False, nan=0.0)\n",
        "\n",
        "        return data, np.array(timesteps)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        return None, None\n",
        "\n",
        "class infoImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.columns = ['sex','age','height','weight','bmi']\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "\n",
        "        data = pd.read_csv(self.directory+'info_vitaldb/cases.csv', index_col=0)\n",
        "        data = data[self.columns].loc[dataset_index].to_numpy()\n",
        "\n",
        "        sex = np.where(data[:, 0] == 'F', -0.5, 0.5)\n",
        "\n",
        "        data = data[:,1:].astype(float)\n",
        "        data = np.c_[sex, normalization(data)]\n",
        "\n",
        "        return data, None\n",
        "\n",
        "class VitalImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.tracks = []\n",
        "        self.filter = [0,0,0]\n",
        "        self.name = 'Vital'\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "\n",
        "        vitaldata = vf.VitalFile(ipath = filepath, track_names = self.tracks)\n",
        "\n",
        "        data = vitaldata.to_pandas(track_names=self.tracks,interval=self.interval)\n",
        "        data = data + 0.00001 # adds small value to avoid mix up with padding values\n",
        "        data = outlierfilter(data, threshhold = self.filter[0] , iterations = 2, min = self.filter[1], max = self.filter[2])\n",
        "\n",
        "        return data, self.name\n",
        "\n",
        "class BPImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        pressureWave = vf.VitalFile(filepath).to_numpy(['SNUADC/ART'], 1/500)\n",
        "\n",
        "        samples = self.interval * 500\n",
        "\n",
        "        # Remove values which derivative is too large\n",
        "        gradient = np.diff(pressureWave,n=1, axis=0, append=0)\n",
        "        gradientfilter1 = ndimage.binary_dilation(np.abs(gradient) > 4,iterations=30)\n",
        "        gradientfilter2 = ndimage.binary_dilation(np.abs(gradient) > 7,iterations=1000)\n",
        "        pressureWave[gradientfilter1] = np.nan\n",
        "        pressureWave[gradientfilter2] = np.nan\n",
        "\n",
        "        # Remove the negative values and values above 250\n",
        "        pressureWave[pressureWave <= 20] = np.nan\n",
        "        pressureWave[pressureWave > 250] = np.nan\n",
        "\n",
        "        pressureWave = self.imputer1.fit_transform(pressureWave)\n",
        "\n",
        "        ### Reshape the pressureWave to 1000 samples (2 seconds) per row\n",
        "        #if (pressureWave.shape[0] % samples) != 0 :\n",
        "        #    steps2fill = samples - (pressureWave.shape[0] % samples)\n",
        "        #    pressureWave = np.pad(array=pressureWave, pad_width=((0,steps2fill),(0,0)), mode='constant', constant_values=np.nan)\n",
        "        length = pressureWave.shape[0] - (pressureWave.shape[0] % samples)\n",
        "        pressureWave = pressureWave[0:length]\n",
        "        return pressureWave.reshape(-1,samples), 'Blood Pressure'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7QPUKpAQ4g3i"
      },
      "outputs": [],
      "source": [
        "###### Create Dataset\n",
        "if create_dataset:\n",
        "    bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bis.name = 'Bispektralindex'\n",
        "    bis.tracks = ['BIS/BIS']\n",
        "    bis.filter = [20, 10, 100]\n",
        "    bis.generateDataset(normalization=NormNone)\n",
        "    bis.save('00_bis.npz')\n",
        "\n",
        "    info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    info.generateDataset(normalization=NormStandard)\n",
        "    info.save('01_info.npz')\n",
        "\n",
        "    bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bloodpressure.name = 'bloodpressure'\n",
        "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
        "    bloodpressure.filter = [20, 20, 250]\n",
        "    bloodpressure.generateDataset(normalization=NormStandard)\n",
        "    bloodpressure.save('02_bloodpressure.npz')\n",
        "\n",
        "    etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    etCO2.name = 'End Tidal CO2'\n",
        "    etCO2.tracks = ['Primus/ETCO2']\n",
        "    etCO2.filter = [5, 15, 50]\n",
        "    etCO2.generateDataset(normalization=NormStandard)\n",
        "    etCO2.save('02_etCO2.npz')\n",
        "\n",
        "    spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    spO2.name = 'SpO2'\n",
        "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
        "    spO2.filter = [3, 80, 100]\n",
        "    spO2.generateDataset(normalization=NormStandard)\n",
        "    spO2.save('02_spO2.npz')\n",
        "\n",
        "    hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    hr.name = 'Heart Rate'\n",
        "    hr.tracks = ['Solar8000/HR']\n",
        "    hr.filter = [20, 40, 180]\n",
        "    hr.generateDataset(normalization=NormStandard)\n",
        "    hr.save('02_hr.npz')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xebtgcoK4g3i"
      },
      "outputs": [],
      "source": [
        "### Load the datasets\n",
        "bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bis.load('00_bis.npz')\n",
        "\n",
        "info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "info.load('01_info.npz')\n",
        "\n",
        "bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bloodpressure.load('02_bloodpressure.npz')\n",
        "\n",
        "etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "etCO2.load('02_etCO2.npz')\n",
        "\n",
        "spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "spO2.load('02_spO2.npz')\n",
        "\n",
        "hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "hr.load('02_hr.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "auUnKLkV4g3j"
      },
      "outputs": [],
      "source": [
        "########################################## COMBINED MODEL ##########################################\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector, Lambda\n",
        "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError, R2Score\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, SGD\n",
        "from talos.utils import lr_normalizer\n",
        "\n",
        "### Combine the vital data\n",
        "vital_train = np.concatenate([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, hr.train_dataset], axis=2)\n",
        "vital_validation = np.concatenate([bloodpressure.validation_dataset, etCO2.validation_dataset, spO2.validation_dataset, hr.validation_dataset], axis=2)\n",
        "vital_test = np.concatenate([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, hr.test_dataset], axis=2)\n",
        "\n",
        "\n",
        "\n",
        "def simple_model(x_train, y_train, x_val, y_val, params):\n",
        "    ### LSTM layers for the vital data\n",
        "    input_vital = Input(shape=(None, vital_train.shape[2]))\n",
        "    vital_layer = Masking(mask_value=0.0)(input_vital)\n",
        "\n",
        "    ### INFO layers\n",
        "    input_info = Input(shape=(info.train_dataset.shape[1],))\n",
        "    info_layer = RepeatVector(vital_train.shape[1])(input_info)\n",
        "\n",
        "    ## Concatenate the LSTM output with the info layer\n",
        "    comb_layer = Concatenate()([vital_layer, info_layer])\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = Dense(units=128, activation='relu')(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "\n",
        "    output = Dense(units=1, activation=params['output_activation'])(comb_layer)\n",
        "    #output = Lambda(lambda x: x * 100)(output)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[input_vital, input_info], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "\n",
        "    model.compile(optimizer=params['optimizer'](learning_rate=params['lr']),\n",
        "                loss=params['loss'],\n",
        "                metrics=['MeanSquaredError', 'MeanAbsoluteError', 'RootMeanSquaredError']\n",
        "                )\n",
        "\n",
        "    y_train = pd.DataFrame(y_train[:,:,0].T).rolling(min_periods=1,window=params['y_smoothing'], center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
        "\n",
        "    # Train the model\n",
        "    out = model.fit(x_train,\n",
        "                        y_train,\n",
        "                        validation_data=[x_val, y_val],\n",
        "                        epochs=params['epochs'],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        verbose=0\n",
        "                        )\n",
        "    return out, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTpELBH0qpqR",
        "outputId": "a4124f80-a186-46de-e1eb-a35877f1d53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ]
        }
      ],
      "source": [
        "import talos\n",
        "\n",
        "p = {\n",
        "    'lr': [0.005],\n",
        "    'epochs': [30],\n",
        "    'optimizer': [Adam],\n",
        "    'output_activation': [ReLU(max_value=100.0)],\n",
        "    'y_smoothing': [1, 2, 4, 8, 16, 32, 64],\n",
        "    'loss': ['MeanSquaredError'],\n",
        "    'batch_size': [4],\n",
        "}\n",
        "\n",
        "scan_object = talos.Scan(x=[vital_train, info.train_dataset],\n",
        "                         y=bis.train_dataset,\n",
        "                         x_val=[vital_validation, info.validation_dataset],\n",
        "                         y_val=bis.validation_dataset,\n",
        "                         experiment_name='SimpleModel_HyperparameterScan',\n",
        "                         multi_input=True,\n",
        "                         params=p,\n",
        "                         model=simple_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}