{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "### Configuration\n",
    "create_dataset = True\n",
    "\n",
    "def in_google_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Get the platform\n",
    "if in_google_colab():\n",
    "    print(\"Running in Google Colab\")\n",
    "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
    "    !pip install vitaldb\n",
    "    directory = 'anesthesia_anomaly_detection/data/'\n",
    "    create_dataset = False\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    directory = 'data/'\n",
    "\n",
    "### Datasetpath\n",
    "datasetpath = 'dataset01/'\n",
    "vitaldbpath = 'vitaldb_tiva/'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vitaldb as vf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def outlierfilter(data: pd.DataFrame,threshhold: float, iterations: int, min: float, max: float):\n",
    "    \"\"\"\n",
    "    A filter function, which calculates the gradient of a given Pandas DataFram Timeseries\n",
    "    and performs a binary dilation on datapoints which exceed a certain treshhold, to detect\n",
    "    and remove unwanted outliers in the dataset. Additionally all values exceeding a given \n",
    "    min/max value are replaced with np.nan and linearly interpolated with the Pandas interpolate\n",
    "    method.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Timeseries Data\n",
    "        threshhold (float): Gradient thresshold\n",
    "        iterations (int): number of iterations of the binary dilation\n",
    "        min (float): maximum expected value\n",
    "        max (float): minimum expected value\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    gradient = np.diff(data,n=1, axis=0, append=0)\n",
    "    gradientfilter = ndimage.binary_dilation(np.abs(gradient) > threshhold, iterations=iterations)\n",
    "\n",
    "    # Apply Filter\n",
    "    data[gradientfilter] = np.nan\n",
    "\n",
    "    data[data <= min] = np.nan\n",
    "    data[data > max] = np.nan\n",
    "\n",
    "    data = data.interpolate(method = 'linear')\n",
    "    data = data.bfill()\n",
    "    return data\n",
    "\n",
    "### Custom Normalization Functions\n",
    "\n",
    "def NormStandard(dataset: np.array):\n",
    "    mean = np.nanmean(dataset)\n",
    "    std = np.nanstd(dataset)\n",
    "    dataset = (dataset - mean) / std\n",
    "    return dataset\n",
    "\n",
    "def NormMinMax(dataset: np.array):\n",
    "    min = np.min(dataset)\n",
    "    max = np.max(dataset)\n",
    "    return (dataset - min) / (max - min)\n",
    "\n",
    "def NormCustomBIS(dataset: np.array):\n",
    "    (100 - dataset) / 100\n",
    "    return dataset\n",
    "\n",
    "def NormNone(dataset: np.array):\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DatasetImport():\n",
    "    def __init__(self, directory: str, dataset: str, vitalpath: str, interval: int = 10):\n",
    "        self.directory = directory\n",
    "        self.datasetpath = directory + dataset\n",
    "        self.vitalpath = directory + vitalpath\n",
    "\n",
    "        self.interval = interval\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "        self.index = pd.read_csv(self.datasetpath +'dataset.csv', index_col=0).index.to_numpy()\n",
    "        \n",
    "    def save(self, filename: str):\n",
    "        np.savez_compressed(self.datasetpath+filename,\n",
    "                            train = self.train_dataset,\n",
    "                            test = self.test_dataset,\n",
    "                            timesteps = self.timesteps,\n",
    "                            )\n",
    "\n",
    "    def load(self, filename: str):\n",
    "        data = np.load(self.datasetpath+filename)\n",
    "        self.train_dataset = data['train']\n",
    "        self.test_dataset = data['test']\n",
    "        try:\n",
    "            self.timesteps = data['timesteps']\n",
    "        except:\n",
    "            self.timesteps = []\n",
    "\n",
    "    def split(self,data):\n",
    "       return train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    def generateDataset(self, normalization):\n",
    "\n",
    "        dataset, self.timesteps = self.generate(self.index, normalization)\n",
    "        \n",
    "        self.train_dataset, self.test_dataset = self.split(dataset)\n",
    "        print('Dataset succesfully generated                 ')\n",
    "\n",
    "    def generate(self, dataset_index: list, normalization):\n",
    "        batch_list = []\n",
    "        timesteps = []\n",
    "\n",
    "        for i, caseid in enumerate(dataset_index):\n",
    "            filepath = self.vitalpath+str(caseid).zfill(4)+'.vital'\n",
    "            data, importName = self.importFunction(filepath)\n",
    "            timesteps.append(data.shape[0])\n",
    "            batch_list.append(data)\n",
    "            print(importName + \" Fortschritt: %.1f\" % (100 * (i+1) / len(dataset_index)),' % ', end='\\r')\n",
    "\n",
    "        ### Pad the dataset\n",
    "        data = tf.keras.preprocessing.sequence.pad_sequences(batch_list, padding='post', dtype='float32', value=0.0)\n",
    "\n",
    "        # Remove 0.0 padded values\n",
    "        data[data == 0.0] = np.nan\n",
    "\n",
    "        # Nomalization\n",
    "        data = normalization(data)\n",
    "\n",
    "        # restore padded values\n",
    "        np.nan_to_num(data, copy=False, nan=0.0)\n",
    "\n",
    "        return data, np.array(timesteps)\n",
    "\n",
    "    def importFunction(self, filepath: str):\n",
    "        return None, None\n",
    "\n",
    "class infoImport(DatasetImport):\n",
    "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
    "        super().__init__(directory,dataset,vitalpath)\n",
    "\n",
    "        self.columns = ['sex','age','height','weight','bmi']\n",
    "\n",
    "    def generate(self, dataset_index: list, normalization):\n",
    "\n",
    "        data = pd.read_csv(self.directory+'info_vitaldb/cases.csv', index_col=0)\n",
    "        data = data[self.columns].loc[dataset_index].to_numpy()\n",
    "        \n",
    "        sex = np.where(data[:, 0] == 'F', -0.5, 0.5)\n",
    "\n",
    "        data = data[:,1:].astype(float)\n",
    "        data = np.c_[sex, normalization(data)]\n",
    "\n",
    "        return data, None\n",
    "\n",
    "class VitalImport(DatasetImport):\n",
    "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
    "        super().__init__(directory,dataset,vitalpath)\n",
    "\n",
    "        self.tracks = []\n",
    "        self.filter = [0,0,0]\n",
    "        self.name = 'Vital'\n",
    "\n",
    "    def importFunction(self, filepath: str):\n",
    "\n",
    "        vitaldata = vf.VitalFile(ipath = filepath, track_names = self.tracks)\n",
    "\n",
    "        data = vitaldata.to_pandas(track_names=self.tracks,interval=self.interval)\n",
    "        data = data + 0.00001 # adds small value to avoid mix up with padding values\n",
    "        data = outlierfilter(data, threshhold = self.filter[0] , iterations = 2, min = self.filter[1], max = self.filter[2])\n",
    "\n",
    "        return data, self.name\n",
    "\n",
    "class BPImport(DatasetImport):\n",
    "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
    "        super().__init__(directory,dataset,vitalpath)\n",
    "\n",
    "    def importFunction(self, filepath: str):\n",
    "        pressureWave = vf.VitalFile(filepath).to_numpy(['SNUADC/ART'], 1/500)\n",
    "\n",
    "        samples = self.interval * 500\n",
    "\n",
    "        # Remove values which derivative is too large\n",
    "        gradient = np.diff(pressureWave,n=1, axis=0, append=0)\n",
    "        gradientfilter1 = ndimage.binary_dilation(np.abs(gradient) > 4,iterations=30)\n",
    "        gradientfilter2 = ndimage.binary_dilation(np.abs(gradient) > 7,iterations=1000)\n",
    "        pressureWave[gradientfilter1] = np.nan\n",
    "        pressureWave[gradientfilter2] = np.nan\n",
    "\n",
    "        # Remove the negative values and values above 250\n",
    "        pressureWave[pressureWave <= 20] = np.nan\n",
    "        pressureWave[pressureWave > 250] = np.nan\n",
    "\n",
    "        pressureWave = self.imputer1.fit_transform(pressureWave)\n",
    "\n",
    "        ### Reshape the pressureWave to 1000 samples (2 seconds) per row\n",
    "        #if (pressureWave.shape[0] % samples) != 0 :\n",
    "        #    steps2fill = samples - (pressureWave.shape[0] % samples)\n",
    "        #    pressureWave = np.pad(array=pressureWave, pad_width=((0,steps2fill),(0,0)), mode='constant', constant_values=np.nan)\n",
    "        length = pressureWave.shape[0] - (pressureWave.shape[0] % samples)\n",
    "        pressureWave = pressureWave[0:length]\n",
    "        return pressureWave.reshape(-1,samples), 'Blood Pressure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset succesfully generated                 \n",
      "Dataset succesfully generated                 \n",
      "Dataset succesfully generated                 \n",
      "Dataset succesfully generated                 \n",
      "Dataset succesfully generated                 \n",
      "Dataset succesfully generated                 \n"
     ]
    }
   ],
   "source": [
    "###### Create Dataset\n",
    "if create_dataset:\n",
    "    bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "    bis.name = 'Bispektralindex'\n",
    "    bis.tracks = ['BIS/BIS']\n",
    "    bis.filter = [20, 0, 100]\n",
    "    bis.generateDataset(normalization=NormCustomBIS)\n",
    "    bis.save('00_bis.npz')\n",
    "\n",
    "    info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "    info.generateDataset(normalization=NormStandard)\n",
    "    info.save('01_info.npz')\n",
    "\n",
    "    bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "    bloodpressure.name = 'bloodpressure'\n",
    "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
    "    bloodpressure.filter = [20, 20, 250]\n",
    "    bloodpressure.generateDataset(normalization=NormStandard)\n",
    "    bloodpressure.save('02_bloodpressure.npz')\n",
    "\n",
    "    etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "    etCO2.name = 'End Tidal CO2'\n",
    "    etCO2.tracks = ['Primus/ETCO2']\n",
    "    etCO2.filter = [5, 15, 50]\n",
    "    etCO2.generateDataset(normalization=NormStandard)\n",
    "    etCO2.save('02_etCO2.npz')\n",
    "\n",
    "    spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "    spO2.name = 'SpO2'\n",
    "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
    "    spO2.filter = [3, 80, 100]\n",
    "    spO2.generateDataset(normalization=NormStandard)\n",
    "    spO2.save('02_spO2.npz')\n",
    "\n",
    "    hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "    hr.name = 'Heart Rate'\n",
    "    hr.tracks = ['Solar8000/HR']\n",
    "    hr.filter = [20, 40, 180]\n",
    "    hr.generateDataset(normalization=NormStandard)\n",
    "    hr.save('02_hr.npz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the datasets\n",
    "bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "bis.load('00_bis.npz')\n",
    "\n",
    "info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "info.load('01_info.npz')\n",
    "\n",
    "bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "bloodpressure.load('02_bloodpressure.npz')\n",
    "\n",
    "etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "etCO2.load('02_etCO2.npz')\n",
    "\n",
    "spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "spO2.load('02_spO2.npz')\n",
    "\n",
    "hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
    "hr.load('02_hr.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## COMBINED MODEL ##########################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector\n",
    "\n",
    "### Combine the vital data\n",
    "vital_train = np.concatenate([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, hr.train_dataset], axis=2) \n",
    "vital_test = np.concatenate([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, hr.test_dataset], axis=2)\n",
    "\n",
    "### LSTM layers for the vital data\n",
    "input_vital = Input(shape=(None, vital_train.shape[2]))\n",
    "vital_layer = Masking(mask_value=999.0)(input_vital)\n",
    "vital_layer = Dense(units=32, activation='sigmoid')(vital_layer)\n",
    "vital_layer = BatchNormalization()(vital_layer)\n",
    "vital_layer = Dense(units=256, activation='sigmoid')(vital_layer)\n",
    "vital_layer = BatchNormalization()(vital_layer)\n",
    "vital_layer = Dense(units=128, activation='sigmoid')(vital_layer)\n",
    "vital_layer = BatchNormalization()(vital_layer)\n",
    "vital_layer = LSTM(units=128, return_sequences=True)(vital_layer)\n",
    "vital_layer = BatchNormalization()(vital_layer)\n",
    "vital_layer = LSTM(units=128, return_sequences=True)(vital_layer)\n",
    "vital_layer = BatchNormalization()(vital_layer)\n",
    "\n",
    "### INFO layers\n",
    "input_info = Input(shape=(info.train_dataset.shape[1],))\n",
    "info_layer = Dense(units=32, activation='relu')(input_info)\n",
    "info_layer = BatchNormalization()(info_layer)\n",
    "info_layer = Dense(units=64, activation='relu')(input_info)\n",
    "info_layer = BatchNormalization()(info_layer)\n",
    "info_layer = Dense(units=32, activation='relu')(input_info)\n",
    "info_layer = BatchNormalization()(info_layer)\n",
    "\n",
    "info_layer = RepeatVector(vital_train.shape[1])(info_layer)\n",
    "\n",
    "## Concatenate the LSTM output with the info layer\n",
    "comb_layer = Concatenate()([vital_layer, info_layer])\n",
    "comb_layer = Dense(units=256, activation='relu')(comb_layer)\n",
    "comb_layer = BatchNormalization()(comb_layer)\n",
    "comb_layer = LSTM(units=256, return_sequences=True)(comb_layer)\n",
    "comb_layer = BatchNormalization()(comb_layer)\n",
    "comb_layer = LSTM(units=128, return_sequences=True)(comb_layer)\n",
    "comb_layer = BatchNormalization()(comb_layer)\n",
    "comb_layer = Dense(units=128, activation='relu')(comb_layer)\n",
    "comb_layer = BatchNormalization()(comb_layer)\n",
    "comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
    "comb_layer = BatchNormalization()(comb_layer)\n",
    "\n",
    "\n",
    "output = Dense(units=1)(comb_layer)\n",
    "output = ReLU(max_value=1.0)(output)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_vital, input_info], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()\n",
    "y = pd.DataFrame(bis.train_dataset[:,:,0].T).rolling(min_periods=1,window=60, center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
    "#y = bis.train_dataset\n",
    "# Train the model\n",
    "history = model.fit([vital_train, info.train_dataset], y, epochs=30, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## TRAINING SUMMARY ##########################################\n",
    "\n",
    "# Evaluate the mod\n",
    "plt.figure()\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## TESTING ##########################################\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prediction\n",
    "y_test = []\n",
    "y_pred_test = []\n",
    "\n",
    "fig, axs = plt.subplots(bis.test_dataset.shape[0], 1, figsize=(10, 100))\n",
    "\n",
    "y_pred =  model.predict([vital_test, info.test_dataset])\n",
    "y_pred = - (y_pred * 100) + 100\n",
    "\n",
    "ground_truth = - (bis.test_dataset * 100) + 100\n",
    "\n",
    "test_index = bis.split(np.array(bis.index))[1]\n",
    "timesteps = bis.split(bis.timesteps)[1]\n",
    "\n",
    "for j, filename in enumerate(test_index):\n",
    "    axs[j].plot(ground_truth[j,:,:], label='Ground Truth')\n",
    "    axs[j].plot(y_pred[j,:,:], label='Prediction')\n",
    "    axs[j].legend()\n",
    "    axs[j].set_title('Case ID: ' + str(filename))\n",
    "    axs[j].axis([0,timesteps[j],0,100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-non-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
