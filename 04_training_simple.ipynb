{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCXc2r4g3c",
        "outputId": "75e4db2a-f2a9-4908-95fe-11751b344fe2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Configuration\n",
        "create_dataset = False\n",
        "\n",
        "def in_google_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Get the platform\n",
        "if in_google_colab():\n",
        "    print(\"Running in Google Colab\")\n",
        "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
        "    !pip install vitaldb\n",
        "    directory = 'anesthesia_anomaly_detection/data/'\n",
        "    create_dataset = False\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    directory = 'data/'\n",
        "\n",
        "### Datasetpath\n",
        "datasetpath = 'dataset01/'\n",
        "vitaldbpath = 'vitaldb_tiva/'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import vitaldb as vf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apfldbrf4g3g"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def outlierfilter(data: pd.DataFrame,threshhold: float, iterations: int, min: float, max: float):\n",
        "    \"\"\"\n",
        "    A filter function, which calculates the gradient of a given Pandas DataFram Timeseries\n",
        "    and performs a binary dilation on datapoints which exceed a certain treshhold, to detect\n",
        "    and remove unwanted outliers in the dataset. Additionally all values exceeding a given\n",
        "    min/max value are replaced with np.nan and linearly interpolated with the Pandas interpolate\n",
        "    method.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Timeseries Data\n",
        "        threshhold (float): Gradient thresshold\n",
        "        iterations (int): number of iterations of the binary dilation\n",
        "        min (float): maximum expected value\n",
        "        max (float): minimum expected value\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: _description_\n",
        "    \"\"\"\n",
        "    gradient = np.diff(data,n=1, axis=0, append=0)\n",
        "    gradientfilter = ndimage.binary_dilation(np.abs(gradient) > threshhold, iterations=iterations)\n",
        "\n",
        "    # Apply Filter\n",
        "    data[gradientfilter] = np.nan\n",
        "\n",
        "    data[data <= min] = np.nan\n",
        "    data[data > max] = np.nan\n",
        "\n",
        "    data = data.interpolate(method = 'linear')\n",
        "    data = data.bfill()\n",
        "    return data\n",
        "\n",
        "### Custom Normalization Functions\n",
        "\n",
        "def NormStandard(dataset: np.array):\n",
        "    mean = np.nanmean(dataset)\n",
        "    std = np.nanstd(dataset)\n",
        "    return (dataset - mean) / std\n",
        "\n",
        "def NormMinMax(dataset: np.array):\n",
        "    min = np.min(dataset)\n",
        "    max = np.max(dataset)\n",
        "    return (dataset - min) / (max - min)\n",
        "\n",
        "def NormCustomBIS(dataset: np.array):\n",
        "    return (100 - dataset) / 100\n",
        "\n",
        "def NormNone(dataset: np.array):\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDGlya8F4g3h"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DatasetImport():\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str, interval: int = 10):\n",
        "        self.directory = directory\n",
        "        self.datasetpath = directory + dataset\n",
        "        self.vitalpath = directory + vitalpath\n",
        "\n",
        "        self.interval = interval\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "        self.index = pd.read_csv(self.datasetpath +'dataset.csv', index_col=0).index.to_numpy()\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        np.savez_compressed(self.datasetpath+filename,\n",
        "                            train = self.train_dataset,\n",
        "                            validation = self.validation_dataset,\n",
        "                            test = self.test_dataset,\n",
        "                            timesteps = self.timesteps,\n",
        "                            )\n",
        "\n",
        "    def load(self, filename: str):\n",
        "        data = np.load(self.datasetpath+filename)\n",
        "        self.train_dataset = data['train']\n",
        "        self.validation_dataset = data['validation']\n",
        "        self.test_dataset = data['test']\n",
        "        try:\n",
        "            self.timesteps = data['timesteps']\n",
        "        except:\n",
        "            self.timesteps = []\n",
        "\n",
        "    def split(self,data):\n",
        "       train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
        "       train, validation = train_test_split(train, test_size=0.15, random_state=42)\n",
        "       return train, validation, test\n",
        "\n",
        "    def generateDataset(self, normalization):\n",
        "\n",
        "        dataset, self.timesteps = self.generate(self.index, normalization)\n",
        "\n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset = self.split(dataset)\n",
        "        print('Dataset succesfully generated                 ')\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "        batch_list = []\n",
        "        timesteps = []\n",
        "\n",
        "        for i, caseid in enumerate(dataset_index):\n",
        "            filepath = self.vitalpath+str(caseid).zfill(4)+'.vital'\n",
        "            data, importName = self.importFunction(filepath)\n",
        "            timesteps.append(data.shape[0])\n",
        "            batch_list.append(data)\n",
        "            print(importName + \" Fortschritt: %.1f\" % (100 * (i+1) / len(dataset_index)),' % ', end='\\r')\n",
        "\n",
        "        ### Pad the dataset\n",
        "        data = tf.keras.preprocessing.sequence.pad_sequences(batch_list, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "        # Remove 0.0 padded values\n",
        "        data[data == 0.0] = np.nan\n",
        "\n",
        "        # Nomalization\n",
        "        data = normalization(data)\n",
        "\n",
        "        # restore padded values\n",
        "        np.nan_to_num(data, copy=False, nan=0.0)\n",
        "\n",
        "        return data, np.array(timesteps)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        return None, None\n",
        "\n",
        "class infoImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.columns = ['sex','age','height','weight','bmi']\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "\n",
        "        data = pd.read_csv(self.directory+'info_vitaldb/cases.csv', index_col=0)\n",
        "        data = data[self.columns].loc[dataset_index].to_numpy()\n",
        "\n",
        "        sex = np.where(data[:, 0] == 'F', -0.5, 0.5)\n",
        "\n",
        "        data = data[:,1:].astype(float)\n",
        "        data = np.c_[sex, normalization(data)]\n",
        "\n",
        "        return data, None\n",
        "\n",
        "class VitalImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.tracks = []\n",
        "        self.filter = [0,0,0]\n",
        "        self.name = 'Vital'\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "\n",
        "        vitaldata = vf.VitalFile(ipath = filepath, track_names = self.tracks)\n",
        "\n",
        "        data = vitaldata.to_pandas(track_names=self.tracks,interval=self.interval)\n",
        "        data = data + 0.00001 # adds small value to avoid mix up with padding values\n",
        "        data = outlierfilter(data, threshhold = self.filter[0] , iterations = 2, min = self.filter[1], max = self.filter[2])\n",
        "\n",
        "        return data, self.name\n",
        "\n",
        "class BPImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        pressureWave = vf.VitalFile(filepath).to_numpy(['SNUADC/ART'], 1/500)\n",
        "\n",
        "        samples = self.interval * 500\n",
        "\n",
        "        # Remove values which derivative is too large\n",
        "        gradient = np.diff(pressureWave,n=1, axis=0, append=0)\n",
        "        gradientfilter1 = ndimage.binary_dilation(np.abs(gradient) > 4,iterations=30)\n",
        "        gradientfilter2 = ndimage.binary_dilation(np.abs(gradient) > 7,iterations=1000)\n",
        "        pressureWave[gradientfilter1] = np.nan\n",
        "        pressureWave[gradientfilter2] = np.nan\n",
        "\n",
        "        # Remove the negative values and values above 250\n",
        "        pressureWave[pressureWave <= 20] = np.nan\n",
        "        pressureWave[pressureWave > 250] = np.nan\n",
        "\n",
        "        pressureWave = self.imputer1.fit_transform(pressureWave)\n",
        "\n",
        "        ### Reshape the pressureWave to 1000 samples (2 seconds) per row\n",
        "        #if (pressureWave.shape[0] % samples) != 0 :\n",
        "        #    steps2fill = samples - (pressureWave.shape[0] % samples)\n",
        "        #    pressureWave = np.pad(array=pressureWave, pad_width=((0,steps2fill),(0,0)), mode='constant', constant_values=np.nan)\n",
        "        length = pressureWave.shape[0] - (pressureWave.shape[0] % samples)\n",
        "        pressureWave = pressureWave[0:length]\n",
        "        return pressureWave.reshape(-1,samples), 'Blood Pressure'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QPUKpAQ4g3i"
      },
      "outputs": [],
      "source": [
        "###### Create Dataset\n",
        "if create_dataset:\n",
        "    bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bis.name = 'Bispektralindex'\n",
        "    bis.tracks = ['BIS/BIS']\n",
        "    bis.filter = [20, 10, 100]\n",
        "    bis.generateDataset(normalization=NormCustomBIS)\n",
        "    bis.save('00_bis.npz')\n",
        "\n",
        "    info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    info.generateDataset(normalization=NormStandard)\n",
        "    info.save('01_info.npz')\n",
        "\n",
        "    bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bloodpressure.name = 'bloodpressure'\n",
        "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
        "    bloodpressure.filter = [20, 20, 250]\n",
        "    bloodpressure.generateDataset(normalization=NormStandard)\n",
        "    bloodpressure.save('02_bloodpressure.npz')\n",
        "\n",
        "    etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    etCO2.name = 'End Tidal CO2'\n",
        "    etCO2.tracks = ['Primus/ETCO2']\n",
        "    etCO2.filter = [5, 15, 50]\n",
        "    etCO2.generateDataset(normalization=NormStandard)\n",
        "    etCO2.save('02_etCO2.npz')\n",
        "\n",
        "    spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    spO2.name = 'SpO2'\n",
        "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
        "    spO2.filter = [3, 80, 100]\n",
        "    spO2.generateDataset(normalization=NormStandard)\n",
        "    spO2.save('02_spO2.npz')\n",
        "\n",
        "    hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    hr.name = 'Heart Rate'\n",
        "    hr.tracks = ['Solar8000/HR']\n",
        "    hr.filter = [20, 40, 180]\n",
        "    hr.generateDataset(normalization=NormStandard)\n",
        "    hr.save('02_hr.npz')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xebtgcoK4g3i"
      },
      "outputs": [],
      "source": [
        "### Load the datasets\n",
        "bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bis.load('00_bis.npz')\n",
        "\n",
        "info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "info.load('01_info.npz')\n",
        "\n",
        "bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bloodpressure.load('02_bloodpressure.npz')\n",
        "\n",
        "etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "etCO2.load('02_etCO2.npz')\n",
        "\n",
        "spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "spO2.load('02_spO2.npz')\n",
        "\n",
        "hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "hr.load('02_hr.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auUnKLkV4g3j",
        "outputId": "72355e3b-2a95-49fa-e18e-25e0cbb572b7"
      },
      "outputs": [],
      "source": [
        "########################################## COMBINED MODEL ##########################################\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
        "\n",
        "### LSTM layers for the blood pressure data\n",
        "\n",
        "### Combine the vital data\n",
        "vital_train = np.concatenate([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, hr.train_dataset], axis=2)\n",
        "vital_validation = np.concatenate([bloodpressure.validation_dataset, etCO2.validation_dataset, spO2.validation_dataset, hr.validation_dataset], axis=2)\n",
        "vital_test = np.concatenate([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, hr.test_dataset], axis=2)\n",
        "\n",
        "### LSTM layers for the vital data\n",
        "input_vital = Input(shape=(None, vital_train.shape[2]))\n",
        "vital_layer = Masking(mask_value=0.0)(input_vital)\n",
        "\n",
        "### INFO layers\n",
        "input_info = Input(shape=(info.train_dataset.shape[1],))\n",
        "\n",
        "info_layer = RepeatVector(vital_train.shape[1])(input_info)\n",
        "\n",
        "## Concatenate the LSTM output with the info layer\n",
        "comb_layer = Concatenate()([vital_layer, info_layer])\n",
        "comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "comb_layer = BatchNormalization()(comb_layer)\n",
        "comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "comb_layer = BatchNormalization()(comb_layer)\n",
        "comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "comb_layer = BatchNormalization()(comb_layer)\n",
        "comb_layer = Dense(units=128, activation='relu')(comb_layer)\n",
        "comb_layer = BatchNormalization()(comb_layer)\n",
        "comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
        "comb_layer = BatchNormalization()(comb_layer)\n",
        "\n",
        "\n",
        "output = Dense(units=1)(comb_layer)\n",
        "output = ReLU(max_value=1.0)(output)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_vital, input_info], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              #loss=tf.keras.losses.Huber(),\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['MeanSquaredError','MeanAbsoluteError','RootMeanSquaredError']\n",
        "              )\n",
        "\n",
        "#model.summary()\n",
        "y = pd.DataFrame(bis.train_dataset[:,:,0].T).rolling(min_periods=1,window=30, center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([vital_train, info.train_dataset],\n",
        "                    y,\n",
        "                    validation_data=([vital_validation, info.validation_dataset], bis.validation_dataset),\n",
        "                    epochs=3,\n",
        "                    batch_size=16\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "6J0t32kS4g3j",
        "outputId": "b4ea6082-d4ff-49ba-f3f3-bfef50f29277"
      },
      "outputs": [],
      "source": [
        "########################################## TRAINING SUMMARY ##########################################\n",
        "\n",
        "# Evaluate the mod\n",
        "train_score = history.history\n",
        "# Plot configuration\n",
        "plt.figure(figsize=(15/2.54, 8/2.54))\n",
        "\n",
        "# Actual plot\n",
        "plt.plot(train_score['loss'], label='Training Loss', color='k', marker='^', markerfacecolor='k')\n",
        "plt.plot(train_score['val_loss'], label='Validation Loss', color='k', marker='s', markerfacecolor='w')\n",
        "\n",
        "# Title and labels\n",
        "#plt.title('Training Model loss')\n",
        "plt.xlabel('Training Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.yscale(\"log\")\n",
        "plt.xlim(0, 30)\n",
        "plt.legend()\n",
        "\n",
        "# Axis settings\n",
        "ax = plt.gca()\n",
        "ax.spines['left'].set_linewidth(1.5)\n",
        "ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "\n",
        "\n",
        "plt.grid(True, linewidth=1.0)\n",
        "\n",
        "#plt.savefig('figure.eps', format='eps')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8tKp7Jsz4g3k",
        "outputId": "7a713b80-8acc-4b03-8595-73594770f48a"
      },
      "outputs": [],
      "source": [
        "########################################## TESTING ##########################################\n",
        "import matplotlib.pyplot as plt\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Prediction\n",
        "y_test = []\n",
        "y_pred_test = []\n",
        "\n",
        "y_pred =  model.predict([vital_test, info.test_dataset])\n",
        "y_pred = - (y_pred * 100) + 100\n",
        "\n",
        "ground_truth = - (bis.test_dataset * 100) + 100\n",
        "ground_truth[ground_truth == 0] = np.nan\n",
        "\n",
        "baseline = NormCustomBIS((bis.test_dataset != 0.0) * 41.0)\n",
        "baseline[baseline == 1.0] = np.nan\n",
        "baseline_score = [0, np.nanmean(np.square(bis.test_dataset - baseline)),      # MSE\n",
        "                  np.nanmean(np.abs(bis.test_dataset - baseline)),            # MAE\n",
        "                  np.sqrt(np.nanmean(np.square(baseline - bis.test_dataset))) # RMSE\n",
        "                  ]\n",
        "\n",
        "test_index = bis.split(np.array(bis.index))[2]\n",
        "timesteps = bis.split(bis.timesteps)[2]\n",
        "\n",
        "### Evaluate Test Score\n",
        "test_score = model.evaluate([vital_test, info.test_dataset], bis.test_dataset, verbose=False)\n",
        "\n",
        "t = PrettyTable(['Metric', 'Train Score', 'Test Score', 'Baseline'], digits = 2)\n",
        "for i, metric in enumerate(model.metrics_names):\n",
        "  if metric == 'loss': continue\n",
        "  t.add_row([metric, train_score[metric][-1], test_score[i], baseline_score[i]])\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timesteps[j]\n",
        "j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "np.where(ground_truth[j] == 100)[0][0]  * 10 / 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "j = 8\n",
        "time = np.arange(0, y_pred[j].shape[0]) * 10 / 60\n",
        "\n",
        "plt.figure(figsize=(14/2.54, 6/2.54))\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Colors\n",
        "color1 = (0, 0, 0)\n",
        "color2 = (1, 1, 1)\n",
        "color3 = (159/255, 182/255, 196/255)\n",
        "color4 = (125/255, 102/255, 102/255)\n",
        "color5 = (153/255, 0, 0)\n",
        "\n",
        "plt.plot(time,y_pred[j], label='Prediction', color='k')\n",
        "plt.plot(time,ground_truth[j], label='Ground Truth', color='r')\n",
        "plt.plot(time,-(baseline[j,:,:] * 100) + 100, label='Baseline', color='b')\n",
        "\n",
        "plt.title('CaseID: '+str(test_index[j]))\n",
        "plt.xlabel('Operationszeit $t_{OP}$')\n",
        "plt.ylabel('Bispektralindex $BIS$')\n",
        "\n",
        "plt.legend(loc='lower center', mode=\"expand\", ncol=3)\n",
        "\n",
        "# Axis settings\n",
        "ax = plt.gca()\n",
        "ax.spines['left'].set_linewidth(1.5)\n",
        "ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "# Limits\n",
        "ax.set_ylim(bottom=0, top=100)\n",
        "ax.set_xlim(left=0, right= 250) \n",
        "\n",
        "# Einheiten auf x-Achse\n",
        "xunit = 'min'\n",
        "ticks = ax.get_xticks()\n",
        "ticks = [int(tick) for tick in ticks]\n",
        "ticks_with_units = [xunit if i == len(ticks) - 2 else ticks[i] for i in range(len(ticks))]\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_xticklabels(ticks_with_units)\n",
        "\n",
        "plt.grid(True, linewidth=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Plot Test Results\n",
        "fig, axs = plt.subplots(bis.test_dataset.shape[0], 1, figsize=(10, 100))\n",
        "\n",
        "for j, filename in enumerate(test_index):\n",
        "    axs[j].plot(ground_truth[j,:,:], label='Ground Truth')\n",
        "    axs[j].plot(y_pred[j,:,:], label='Prediction')\n",
        "    axs[j].plot(-(baseline[j,:,:] * 100) + 100, label='Baseline')\n",
        "    axs[j].legend()\n",
        "    axs[j].set_title('Case ID: ' + str(filename))\n",
        "    axs[j].axis([0,timesteps[j],0,100])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
