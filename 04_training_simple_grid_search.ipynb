{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCXc2r4g3c",
        "outputId": "2080f8cd-0367-448a-f397-8666b3031658"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Configuration\n",
        "create_dataset = False\n",
        "\n",
        "def in_google_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Get the platform\n",
        "if in_google_colab():\n",
        "    print(\"Running in Google Colab\")\n",
        "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
        "    !pip install vitaldb astetik chances kerasplotlib statsmodels tqdm wrangle\n",
        "    !pip install --no-deps talos\n",
        "    directory = 'anesthesia_anomaly_detection/data/'\n",
        "    create_dataset = False\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    directory = 'data/'\n",
        "\n",
        "### Datasetpath\n",
        "datasetpath = 'dataset01/'\n",
        "vitaldbpath = 'vitaldb_tiva/'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import vitaldb as vf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apfldbrf4g3g"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def outlierfilter(data: pd.DataFrame,threshhold: float, iterations: int, min: float, max: float):\n",
        "    \"\"\"\n",
        "    A filter function, which calculates the gradient of a given Pandas DataFram Timeseries\n",
        "    and performs a binary dilation on datapoints which exceed a certain treshhold, to detect\n",
        "    and remove unwanted outliers in the dataset. Additionally all values exceeding a given\n",
        "    min/max value are replaced with np.nan and linearly interpolated with the Pandas interpolate\n",
        "    method.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Timeseries Data\n",
        "        threshhold (float): Gradient thresshold\n",
        "        iterations (int): number of iterations of the binary dilation\n",
        "        min (float): maximum expected value\n",
        "        max (float): minimum expected value\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: _description_\n",
        "    \"\"\"\n",
        "    gradient = np.diff(data,n=1, axis=0, append=0)\n",
        "    gradientfilter = ndimage.binary_dilation(np.abs(gradient) > threshhold, iterations=iterations)\n",
        "\n",
        "    # Apply Filter\n",
        "    data[gradientfilter] = np.nan\n",
        "\n",
        "    data[data <= min] = np.nan\n",
        "    data[data > max] = np.nan\n",
        "\n",
        "    data = data.interpolate(method = 'linear')\n",
        "    data = data.bfill()\n",
        "    return data\n",
        "\n",
        "### Custom Normalization Functions\n",
        "\n",
        "def NormStandard(dataset: np.array):\n",
        "    mean = np.nanmean(dataset)\n",
        "    std = np.nanstd(dataset)\n",
        "    return (dataset - mean) / std\n",
        "\n",
        "def NormMinMax(dataset: np.array):\n",
        "    min = np.min(dataset)\n",
        "    max = np.max(dataset)\n",
        "    return (dataset - min) / (max - min)\n",
        "\n",
        "def NormCustomBIS(dataset: np.array):\n",
        "    return (100 - dataset) / 100\n",
        "\n",
        "def NormNone(dataset: np.array):\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDGlya8F4g3h"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DatasetImport():\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str, interval: int = 10):\n",
        "        self.directory = directory\n",
        "        self.datasetpath = directory + dataset\n",
        "        self.vitalpath = directory + vitalpath\n",
        "\n",
        "        self.interval = interval\n",
        "\n",
        "        self.dataset = None\n",
        "        self.validation_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "        self.index = pd.read_csv(self.datasetpath +'dataset.csv', index_col=0).index.to_numpy()\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        np.savez_compressed(self.datasetpath+filename,\n",
        "                            train = self.train_dataset,\n",
        "                            validation = self.validation_dataset,\n",
        "                            test = self.test_dataset,\n",
        "                            timesteps = self.timesteps,\n",
        "                            )\n",
        "\n",
        "    def load(self, filename: str):\n",
        "        data = np.load(self.datasetpath+filename)\n",
        "        self.train_dataset = data['train']\n",
        "        self.validation_dataset = data['validation']\n",
        "        self.test_dataset = data['test']\n",
        "        try:\n",
        "            self.timesteps = data['timesteps']\n",
        "        except:\n",
        "            self.timesteps = []\n",
        "\n",
        "    def split(self,data):\n",
        "       train, test = train_test_split(data, test_size=0.15, random_state=42)\n",
        "       train, validation = train_test_split(train, test_size=0.15, random_state=42)\n",
        "       return train, validation, test\n",
        "\n",
        "    def generateDataset(self, normalization):\n",
        "\n",
        "        dataset, self.timesteps = self.generate(self.index, normalization)\n",
        "\n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset = self.split(dataset)\n",
        "        print('Dataset succesfully generated                 ')\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "        batch_list = []\n",
        "        timesteps = []\n",
        "\n",
        "        for i, caseid in enumerate(dataset_index):\n",
        "            filepath = self.vitalpath+str(caseid).zfill(4)+'.vital'\n",
        "            data, importName = self.importFunction(filepath)\n",
        "            timesteps.append(data.shape[0])\n",
        "            batch_list.append(data)\n",
        "            print(importName + \" Fortschritt: %.1f\" % (100 * (i+1) / len(dataset_index)),' % ', end='\\r')\n",
        "\n",
        "        ### Pad the dataset\n",
        "        data = tf.keras.preprocessing.sequence.pad_sequences(batch_list, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "        # Remove 0.0 padded values\n",
        "        data[data == 0.0] = np.nan\n",
        "\n",
        "        # Nomalization\n",
        "        data = normalization(data)\n",
        "\n",
        "        # restore padded values\n",
        "        np.nan_to_num(data, copy=False, nan=0.0)\n",
        "\n",
        "        return data, np.array(timesteps)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        return None, None\n",
        "\n",
        "class infoImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.columns = ['sex','age','height','weight','bmi']\n",
        "\n",
        "    def generate(self, dataset_index: list, normalization):\n",
        "\n",
        "        data = pd.read_csv(self.directory+'info_vitaldb/cases.csv', index_col=0)\n",
        "        data = data[self.columns].loc[dataset_index].to_numpy()\n",
        "\n",
        "        sex = np.where(data[:, 0] == 'F', -0.5, 0.5)\n",
        "\n",
        "        data = data[:,1:].astype(float)\n",
        "        data = np.c_[sex, normalization(data)]\n",
        "\n",
        "        return data, None\n",
        "\n",
        "class VitalImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "        self.tracks = []\n",
        "        self.filter = [0,0,0]\n",
        "        self.name = 'Vital'\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "\n",
        "        vitaldata = vf.VitalFile(ipath = filepath, track_names = self.tracks)\n",
        "\n",
        "        data = vitaldata.to_pandas(track_names=self.tracks,interval=self.interval)\n",
        "        data = data + 0.00001 # adds small value to avoid mix up with padding values\n",
        "        data = outlierfilter(data, threshhold = self.filter[0] , iterations = 2, min = self.filter[1], max = self.filter[2])\n",
        "\n",
        "        return data, self.name\n",
        "\n",
        "class BPImport(DatasetImport):\n",
        "    def __init__(self, directory: str, dataset: str, vitalpath: str):\n",
        "        super().__init__(directory,dataset,vitalpath)\n",
        "\n",
        "    def importFunction(self, filepath: str):\n",
        "        pressureWave = vf.VitalFile(filepath).to_numpy(['SNUADC/ART'], 1/500)\n",
        "\n",
        "        samples = self.interval * 500\n",
        "\n",
        "        # Remove values which derivative is too large\n",
        "        gradient = np.diff(pressureWave,n=1, axis=0, append=0)\n",
        "        gradientfilter1 = ndimage.binary_dilation(np.abs(gradient) > 4,iterations=30)\n",
        "        gradientfilter2 = ndimage.binary_dilation(np.abs(gradient) > 7,iterations=1000)\n",
        "        pressureWave[gradientfilter1] = np.nan\n",
        "        pressureWave[gradientfilter2] = np.nan\n",
        "\n",
        "        # Remove the negative values and values above 250\n",
        "        pressureWave[pressureWave <= 20] = np.nan\n",
        "        pressureWave[pressureWave > 250] = np.nan\n",
        "\n",
        "        pressureWave = self.imputer1.fit_transform(pressureWave)\n",
        "\n",
        "        ### Reshape the pressureWave to 1000 samples (2 seconds) per row\n",
        "        #if (pressureWave.shape[0] % samples) != 0 :\n",
        "        #    steps2fill = samples - (pressureWave.shape[0] % samples)\n",
        "        #    pressureWave = np.pad(array=pressureWave, pad_width=((0,steps2fill),(0,0)), mode='constant', constant_values=np.nan)\n",
        "        length = pressureWave.shape[0] - (pressureWave.shape[0] % samples)\n",
        "        pressureWave = pressureWave[0:length]\n",
        "        return pressureWave.reshape(-1,samples), 'Blood Pressure'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QPUKpAQ4g3i"
      },
      "outputs": [],
      "source": [
        "###### Create Dataset\n",
        "if create_dataset:\n",
        "    bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bis.name = 'Bispektralindex'\n",
        "    bis.tracks = ['BIS/BIS']\n",
        "    bis.filter = [20, 10, 100]\n",
        "    bis.generateDataset(normalization=NormCustomBIS)\n",
        "    bis.save('00_bis.npz')\n",
        "\n",
        "    info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    info.generateDataset(normalization=NormStandard)\n",
        "    info.save('01_info.npz')\n",
        "\n",
        "    bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bloodpressure.name = 'bloodpressure'\n",
        "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
        "    bloodpressure.filter = [20, 20, 250]\n",
        "    bloodpressure.generateDataset(normalization=NormStandard)\n",
        "    bloodpressure.save('02_bloodpressure.npz')\n",
        "\n",
        "    etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    etCO2.name = 'End Tidal CO2'\n",
        "    etCO2.tracks = ['Primus/ETCO2']\n",
        "    etCO2.filter = [5, 15, 50]\n",
        "    etCO2.generateDataset(normalization=NormStandard)\n",
        "    etCO2.save('02_etCO2.npz')\n",
        "\n",
        "    spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    spO2.name = 'SpO2'\n",
        "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
        "    spO2.filter = [3, 80, 100]\n",
        "    spO2.generateDataset(normalization=NormStandard)\n",
        "    spO2.save('02_spO2.npz')\n",
        "\n",
        "    hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    hr.name = 'Heart Rate'\n",
        "    hr.tracks = ['Solar8000/HR']\n",
        "    hr.filter = [20, 40, 180]\n",
        "    hr.generateDataset(normalization=NormStandard)\n",
        "    hr.save('02_hr.npz')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xebtgcoK4g3i"
      },
      "outputs": [],
      "source": [
        "### Load the datasets\n",
        "bis = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bis.load('00_bis.npz')\n",
        "\n",
        "info = infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "info.load('01_info.npz')\n",
        "\n",
        "bloodpressure = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bloodpressure.load('02_bloodpressure.npz')\n",
        "\n",
        "etCO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "etCO2.load('02_etCO2.npz')\n",
        "\n",
        "spO2 = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "spO2.load('02_spO2.npz')\n",
        "\n",
        "hr = VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "hr.load('02_hr.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auUnKLkV4g3j"
      },
      "outputs": [],
      "source": [
        "########################################## COMBINED MODEL ##########################################\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
        "\n",
        "### Combine the vital data\n",
        "vital_train = np.concatenate([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, hr.train_dataset], axis=2)\n",
        "vital_validation = np.concatenate([bloodpressure.validation_dataset, etCO2.validation_dataset, spO2.validation_dataset, hr.validation_dataset], axis=2)\n",
        "vital_test = np.concatenate([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, hr.test_dataset], axis=2)\n",
        "\n",
        "y_train = pd.DataFrame(bis.train_dataset[:,:,0].T).rolling(min_periods=1,window=30, center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
        "\n",
        "\n",
        "def simple_model(x_train, y_train, x_val, y_val, params):\n",
        "    ### LSTM layers for the vital data\n",
        "    input_vital = Input(shape=(None, vital_train.shape[2]))\n",
        "    vital_layer = Masking(mask_value=0.0)(input_vital)\n",
        "\n",
        "    ### INFO layers\n",
        "    input_info = Input(shape=(info.train_dataset.shape[1],))\n",
        "    info_layer = RepeatVector(vital_train.shape[1])(input_info)\n",
        "\n",
        "    ## Concatenate the LSTM output with the info layer\n",
        "    comb_layer = Concatenate()([vital_layer, info_layer])\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = LSTM(units=32, return_sequences=True)(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = Dense(units=128, activation='relu')(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "    comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
        "    comb_layer = BatchNormalization()(comb_layer)\n",
        "\n",
        "\n",
        "    output = Dense(units=1)(comb_layer)\n",
        "    output = ReLU(max_value=1.0)(output)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[input_vital, input_info], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.MeanSquaredError(),\n",
        "                metrics=['MeanSquaredError','MeanAbsoluteError']\n",
        "                )\n",
        "\n",
        "    #model.summary()\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    out = model.fit(x_train,\n",
        "                        y_train,\n",
        "                        validation_data=[x_val, y_val],\n",
        "                        epochs=30,\n",
        "                        batch_size=params['batch_size']\n",
        "                        verbose=0\n",
        "                        )\n",
        "    return out, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdl4u7SZsk9i",
        "outputId": "b2c4dda6-1a01-430d-c0ab-387f4ff461c7"
      },
      "outputs": [],
      "source": [
        "import talos\n",
        "\n",
        "p = {\n",
        "    'lr': (0.001, 0.01, 0.1, 1, 10),\n",
        "    'epochs': [10, 20, 30],\n",
        "    'optimizer': [Adam, Adagrad, RMSprop],\n",
        "    'batch_size': [64, 32, 16, 8, 4, 2],\n",
        "}\n",
        "\n",
        "scan_object = talos.Scan(x=[vital_train, info.train_dataset],\n",
        "                         y=y_train,\n",
        "                         x_val=[vital_validation, info.validation_dataset],\n",
        "                         y_val=bis.validation_dataset,\n",
        "                         experiment_name='SimpleModel_BatchSize',\n",
        "                         multi_input=True,\n",
        "                         params=p,\n",
        "                         model=simple_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
