{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7rdlib1gg0"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCXc2r4g3c",
        "outputId": "a394c692-bb8f-43fc-cb75-d718ad56024c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Cloning into 'anesthesia_anomaly_detection'...\n",
            "remote: Enumerating objects: 1180, done.\u001b[K\n",
            "remote: Counting objects: 100% (217/217), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 1180 (delta 154), reused 198 (delta 139), pack-reused 963\u001b[K\n",
            "Receiving objects: 100% (1180/1180), 241.60 MiB | 16.25 MiB/s, done.\n",
            "Resolving deltas: 100% (685/685), done.\n",
            "Updating files: 100% (556/556), done.\n",
            "Collecting vitaldb\n",
            "  Downloading vitaldb-1.4.9-py3-none-any.whl.metadata (520 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vitaldb) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.31.0)\n",
            "Collecting wfdb (from vitaldb)\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2024.7.4)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb->vitaldb) (2.22)\n",
            "Downloading vitaldb-1.4.9-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wfdb, vitaldb\n",
            "Successfully installed vitaldb-1.4.9 wfdb-4.1.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Configuration\n",
        "create_dataset = False\n",
        "\n",
        "def in_google_colab():\n",
        "    \"\"\"Checks if the code is running in Google Colab\n",
        "\n",
        "    Returns:\n",
        "        bool: _description_\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if in_google_colab():\n",
        "    print(\"Running in Google Colab\")\n",
        "    # Install necessary packages in Google Colab\n",
        "    !rm -r sample_data/\n",
        "    !git clone https://github.com/naderinho/anesthesia_anomaly_detection\n",
        "    !cp -r anesthesia_anomaly_detection/* .\n",
        "    !rm -r anesthesia_anomaly_detection/\n",
        "    !pip install vitaldb\n",
        "    create_dataset = False\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "\n",
        "### Datasetpath\n",
        "directory = 'data/'\n",
        "datasetpath = 'dataset03/'\n",
        "vitaldbpath = 'vitaldb_sevofl/'\n",
        "\n",
        "### Import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import vitaldb as vf\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "### Custom functions\n",
        "import modules as md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RNq17B1gg4"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7QPUKpAQ4g3i"
      },
      "outputs": [],
      "source": [
        "###### Create Dataset\n",
        "if create_dataset:\n",
        "    bis = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bis.name = 'Bispektralindex'\n",
        "    bis.tracks = ['BIS/BIS']\n",
        "    bis.filter = [20, 10, 100]\n",
        "    bis.generateDataset(normalization=md.NormNone)\n",
        "    bis.save('00_bis.npz')\n",
        "\n",
        "    info = md.infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    info.generateDataset(normalization=md.NormStandard)\n",
        "    info.save('01_info.npz')\n",
        "\n",
        "    bloodpressure = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    bloodpressure.name = 'bloodpressure'\n",
        "    bloodpressure.tracks = ['Solar8000/ART_DBP', 'Solar8000/ART_MBP', 'Solar8000/ART_SBP']\n",
        "    bloodpressure.filter = [20, 20, 250]\n",
        "    bloodpressure.generateDataset(normalization=md.NormStandard)\n",
        "    bloodpressure.save('02_bloodpressure.npz')\n",
        "\n",
        "    etCO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    etCO2.name = 'End Tidal CO2'\n",
        "    etCO2.tracks = ['Primus/ETCO2']\n",
        "    etCO2.filter = [5, 15, 50]\n",
        "    etCO2.generateDataset(normalization=md.NormStandard)\n",
        "    etCO2.save('02_etCO2.npz')\n",
        "\n",
        "    spO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    spO2.name = 'SpO2'\n",
        "    spO2.tracks = ['Solar8000/PLETH_SPO2']\n",
        "    spO2.filter = [3, 80, 100]\n",
        "    spO2.generateDataset(normalization=md.NormStandard)\n",
        "    spO2.save('02_spO2.npz')\n",
        "\n",
        "    hr = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    hr.name = 'Heart Rate'\n",
        "    hr.tracks = ['Solar8000/HR']\n",
        "    hr.filter = [20, 40, 180]\n",
        "    hr.generateDataset(normalization=md.NormStandard)\n",
        "    hr.save('02_hr.npz')\n",
        "\n",
        "    mac = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "    mac.name = 'MAC'\n",
        "    mac.tracks = ['Primus/MAC']\n",
        "    mac.filterON = False\n",
        "    mac.generateDataset(normalization=md.NormNone)\n",
        "    mac.save('03_mac.npz')\n",
        "\n",
        "### Load the datasets\n",
        "bis = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bis.load('00_bis.npz')\n",
        "\n",
        "info = md.infoImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "info.load('01_info.npz')\n",
        "\n",
        "bloodpressure = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "bloodpressure.load('02_bloodpressure.npz')\n",
        "\n",
        "etCO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "etCO2.load('02_etCO2.npz')\n",
        "\n",
        "spO2 = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "spO2.load('02_spO2.npz')\n",
        "\n",
        "hr = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "hr.load('02_hr.npz')\n",
        "\n",
        "mac = md.VitalImport(directory= directory, dataset=datasetpath, vitalpath=vitaldbpath)\n",
        "mac.load('03_mac.npz')\n",
        "\n",
        "\n",
        "train_index, val_index, test_index = bis.split(np.array(bis.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZl7YJXE1gg6"
      },
      "source": [
        "# Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auUnKLkV4g3j",
        "outputId": "7c76f243-9a66-4163-f748-64105e124209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, 3)]            0         []                            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, None, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " masking (Masking)           (None, None, 3)              0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, None, 1)              4         ['input_4[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, None, 64)             17408     ['masking[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_18 (LSTM)              (None, None, 64)             16896     ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, None, 64)             256       ['lstm[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, None, 64)             256       ['lstm_18[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, None, 64)             33024     ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " lstm_19 (LSTM)              (None, None, 64)             33024     ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, None, 64)             256       ['lstm_1[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, None, 64)             256       ['lstm_19[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               (None, None, 64)             33024     ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_20 (LSTM)              (None, None, 64)             33024     ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, None, 64)             256       ['lstm_2[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, None, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, None, 64)             256       ['lstm_20[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, None, 64)             33024     ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_8 (LSTM)               (None, None, 64)             16896     ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_13 (LSTM)              (None, None, 64)             16896     ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_21 (LSTM)              (None, None, 64)             33024     ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, None, 64)             256       ['lstm_3[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, None, 64)             256       ['lstm_8[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, None, 64)             256       ['lstm_13[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, None, 64)             256       ['lstm_21[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)               (None, None, 128)            98816     ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_9 (LSTM)               (None, None, 64)             33024     ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_14 (LSTM)              (None, None, 64)             33024     ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " lstm_22 (LSTM)              (None, None, 128)            98816     ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, None, 128)            512       ['lstm_4[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, None, 64)             256       ['lstm_9[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, None, 64)             256       ['lstm_14[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, None, 128)            512       ['lstm_22[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)               (None, None, 64)             49408     ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_10 (LSTM)              (None, None, 64)             33024     ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_15 (LSTM)              (None, None, 64)             33024     ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " lstm_23 (LSTM)              (None, None, 64)             49408     ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, None, 64)             256       ['lstm_5[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, None, 64)             256       ['lstm_10[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, None, 64)             256       ['lstm_15[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, None, 64)             256       ['lstm_23[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)               (None, None, 16)             5184      ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_11 (LSTM)              (None, None, 16)             5184      ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " lstm_16 (LSTM)              (None, None, 16)             5184      ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " lstm_24 (LSTM)              (None, None, 16)             5184      ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, None, 16)             64        ['lstm_6[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, None, 16)             64        ['lstm_11[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, None, 16)             64        ['lstm_16[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, None, 16)             64        ['lstm_24[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)        [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)               (None, None, 16)             2112      ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " lstm_12 (LSTM)              (None, None, 8)              800       ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " lstm_17 (LSTM)              (None, None, 8)              800       ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " lstm_25 (LSTM)              (None, None, 16)             2112      ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVecto  (None, 3744, 5)              0         ['input_5[0][0]']             \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, None, 16)             64        ['lstm_7[0][0]']              \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, None, 8)              32        ['lstm_12[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, None, 8)              32        ['lstm_17[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, None, 16)             64        ['lstm_25[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 3744, 16)             96        ['repeat_vector[0][0]']       \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 16)             272       ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, None, 8)              72        ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, None, 8)              72        ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, None, 16)             272       ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 3744, 16)             272       ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 3744, 64)             0         ['dense[0][0]',               \n",
            "                                                                     'dense_1[0][0]',             \n",
            "                                                                     'dense_2[0][0]',             \n",
            "                                                                     'dense_3[0][0]',             \n",
            "                                                                     'dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 3744, 64)             4160      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 3744, 64)             4160      ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 3744, 64)             4160      ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 3744, 32)             2080      ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 3744, 16)             528       ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 3744, 1)              17        ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 3744, 1)              0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 743077 (2.83 MB)\n",
            "Trainable params: 740291 (2.82 MB)\n",
            "Non-trainable params: 2786 (10.88 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "########################################## COMBINED MODEL ##########################################\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, ReLU, Dropout, Concatenate, Masking, Conv1D, MaxPooling1D, BatchNormalization, RepeatVector, Lambda\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
        "\n",
        "### Blood Pressure Input\n",
        "input_bp = Input(shape=(None, bloodpressure.train_dataset.shape[2]))\n",
        "bp_layer = Masking(mask_value=0.0)(input_bp)\n",
        "\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=128, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=64, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=16, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = LSTM(units=16, return_sequences=True)(bp_layer)\n",
        "bp_layer = BatchNormalization()(bp_layer)\n",
        "bp_layer = Dense(units=16, activation='linear')(bp_layer)\n",
        "\n",
        "### etCO2 Input\n",
        "input_co2 = Input(shape=(None, etCO2.train_dataset.shape[2]))\n",
        "\n",
        "co2_layer = LSTM(units=64, return_sequences=True)(input_co2)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=64, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=64, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=16, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = LSTM(units=8, return_sequences=True)(co2_layer)\n",
        "co2_layer = BatchNormalization()(co2_layer)\n",
        "co2_layer = Dense(units=8, activation='linear')(co2_layer)\n",
        "\n",
        "### spo2 Input\n",
        "input_spo2 = Input(shape=(None, spO2.train_dataset.shape[2]))\n",
        "\n",
        "spo2_layer = LSTM(units=64, return_sequences=True)(input_spo2)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=64, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=64, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=16, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = LSTM(units=8, return_sequences=True)(spo2_layer)\n",
        "spo2_layer = BatchNormalization()(spo2_layer)\n",
        "spo2_layer = Dense(units=8, activation='linear')(spo2_layer)\n",
        "\n",
        "# Sevoflurane layers\n",
        "input_mac = Input(shape=(None, mac.train_dataset.shape[2]))\n",
        "mac_layer = BatchNormalization()(input_mac)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=128, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=64, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=16, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = LSTM(units=16, return_sequences=True)(mac_layer)\n",
        "mac_layer = BatchNormalization()(mac_layer)\n",
        "mac_layer = Dense(units=16, activation='linear')(mac_layer)\n",
        "\n",
        "### INFO layers\n",
        "input_info = Input(shape=(info.train_dataset.shape[1],))\n",
        "info_layer = RepeatVector(bloodpressure.train_dataset.shape[1])(input_info)\n",
        "info_layer = Dense(units=16, activation='linear')(info_layer)\n",
        "info_layer = Dense(units=16, activation='sigmoid')(info_layer)\n",
        "\n",
        "\n",
        "## Concatenate the Sevoflurane output with the info layer\n",
        "comb_layer = Concatenate()([bp_layer, co2_layer, spo2_layer, mac_layer, info_layer])\n",
        "comb_layer = Dense(units=64, activation='linear')(comb_layer)\n",
        "comb_layer = Dense(units=64, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=64, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=32, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=16, activation='relu')(comb_layer)\n",
        "comb_layer = Dense(units=1, activation=ReLU(max_value=1.0))(comb_layer)\n",
        "output = Lambda(lambda x: x * 100)(comb_layer)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_bp, input_co2, input_spo2, input_mac, input_info], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['MeanSquaredError','MeanAbsoluteError','RootMeanSquaredError']\n",
        "              )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp7Aw3mx1gg7"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT7uctql1gg8",
        "outputId": "4e2d5615-c3ea-4c8a-c330-463058a618a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "47/47 [==============================] - 180s 2s/step - loss: 177.7199 - mean_squared_error: 177.7199 - mean_absolute_error: 6.3309 - root_mean_squared_error: 13.3312 - val_loss: 465.5007 - val_mean_squared_error: 465.5007 - val_mean_absolute_error: 19.4537 - val_root_mean_squared_error: 21.5755\n",
            "Epoch 2/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 55.5040 - mean_squared_error: 55.5040 - mean_absolute_error: 3.5222 - root_mean_squared_error: 7.4501 - val_loss: 465.8743 - val_mean_squared_error: 465.8743 - val_mean_absolute_error: 20.2397 - val_root_mean_squared_error: 21.5841\n",
            "Epoch 3/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 43.4577 - mean_squared_error: 43.4577 - mean_absolute_error: 3.1589 - root_mean_squared_error: 6.5922 - val_loss: 537.1931 - val_mean_squared_error: 537.1931 - val_mean_absolute_error: 21.1144 - val_root_mean_squared_error: 23.1774\n",
            "Epoch 4/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 39.7157 - mean_squared_error: 39.7157 - mean_absolute_error: 3.0227 - root_mean_squared_error: 6.3020 - val_loss: 642.5090 - val_mean_squared_error: 642.5089 - val_mean_absolute_error: 22.3888 - val_root_mean_squared_error: 25.3478\n",
            "Epoch 5/150\n",
            "47/47 [==============================] - 96s 2s/step - loss: 36.5453 - mean_squared_error: 36.5453 - mean_absolute_error: 2.9438 - root_mean_squared_error: 6.0453 - val_loss: 651.2320 - val_mean_squared_error: 651.2319 - val_mean_absolute_error: 22.4288 - val_root_mean_squared_error: 25.5192\n",
            "Epoch 6/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 32.1788 - mean_squared_error: 32.1788 - mean_absolute_error: 2.7332 - root_mean_squared_error: 5.6726 - val_loss: 595.8913 - val_mean_squared_error: 595.8914 - val_mean_absolute_error: 21.2099 - val_root_mean_squared_error: 24.4109\n",
            "Epoch 7/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 32.7607 - mean_squared_error: 32.7607 - mean_absolute_error: 2.7979 - root_mean_squared_error: 5.7237 - val_loss: 353.1109 - val_mean_squared_error: 353.1109 - val_mean_absolute_error: 16.5297 - val_root_mean_squared_error: 18.7912\n",
            "Epoch 8/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 33.1795 - mean_squared_error: 33.1795 - mean_absolute_error: 2.8367 - root_mean_squared_error: 5.7602 - val_loss: 331.6139 - val_mean_squared_error: 331.6139 - val_mean_absolute_error: 16.0962 - val_root_mean_squared_error: 18.2103\n",
            "Epoch 9/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 31.8859 - mean_squared_error: 31.8859 - mean_absolute_error: 2.7670 - root_mean_squared_error: 5.6468 - val_loss: 421.6133 - val_mean_squared_error: 421.6133 - val_mean_absolute_error: 18.0217 - val_root_mean_squared_error: 20.5332\n",
            "Epoch 10/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 30.2957 - mean_squared_error: 30.2957 - mean_absolute_error: 2.6787 - root_mean_squared_error: 5.5042 - val_loss: 464.1204 - val_mean_squared_error: 464.1204 - val_mean_absolute_error: 18.7555 - val_root_mean_squared_error: 21.5435\n",
            "Epoch 11/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 29.1645 - mean_squared_error: 29.1645 - mean_absolute_error: 2.6449 - root_mean_squared_error: 5.4004 - val_loss: 482.0262 - val_mean_squared_error: 482.0262 - val_mean_absolute_error: 19.4051 - val_root_mean_squared_error: 21.9551\n",
            "Epoch 12/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 32.3308 - mean_squared_error: 32.3308 - mean_absolute_error: 2.8260 - root_mean_squared_error: 5.6860 - val_loss: 672.8805 - val_mean_squared_error: 672.8806 - val_mean_absolute_error: 22.2381 - val_root_mean_squared_error: 25.9399\n",
            "Epoch 13/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 31.1206 - mean_squared_error: 31.1206 - mean_absolute_error: 2.7658 - root_mean_squared_error: 5.5786 - val_loss: 618.2734 - val_mean_squared_error: 618.2733 - val_mean_absolute_error: 21.3686 - val_root_mean_squared_error: 24.8651\n",
            "Epoch 14/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 31.9661 - mean_squared_error: 31.9661 - mean_absolute_error: 2.7884 - root_mean_squared_error: 5.6539 - val_loss: 516.5522 - val_mean_squared_error: 516.5522 - val_mean_absolute_error: 19.5568 - val_root_mean_squared_error: 22.7278\n",
            "Epoch 15/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 29.8850 - mean_squared_error: 29.8850 - mean_absolute_error: 2.6738 - root_mean_squared_error: 5.4667 - val_loss: 378.4102 - val_mean_squared_error: 378.4102 - val_mean_absolute_error: 17.0645 - val_root_mean_squared_error: 19.4528\n",
            "Epoch 16/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 30.5723 - mean_squared_error: 30.5723 - mean_absolute_error: 2.7341 - root_mean_squared_error: 5.5292 - val_loss: 672.7427 - val_mean_squared_error: 672.7427 - val_mean_absolute_error: 22.1053 - val_root_mean_squared_error: 25.9373\n",
            "Epoch 17/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 28.6725 - mean_squared_error: 28.6725 - mean_absolute_error: 2.6181 - root_mean_squared_error: 5.3547 - val_loss: 34.8892 - val_mean_squared_error: 34.8892 - val_mean_absolute_error: 3.3620 - val_root_mean_squared_error: 5.9067\n",
            "Epoch 18/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 28.7200 - mean_squared_error: 28.7200 - mean_absolute_error: 2.6729 - root_mean_squared_error: 5.3591 - val_loss: 834.3375 - val_mean_squared_error: 834.3375 - val_mean_absolute_error: 24.7820 - val_root_mean_squared_error: 28.8849\n",
            "Epoch 19/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 28.7713 - mean_squared_error: 28.7713 - mean_absolute_error: 2.6348 - root_mean_squared_error: 5.3639 - val_loss: 29.9125 - val_mean_squared_error: 29.9125 - val_mean_absolute_error: 2.5579 - val_root_mean_squared_error: 5.4692\n",
            "Epoch 20/150\n",
            "47/47 [==============================] - ETA: 0s - loss: 27.7515 - mean_squared_error: 27.7515 - mean_absolute_error: 2.5960 - root_mean_squared_error: 5.2680"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r47/47 [==============================] - 98s 2s/step - loss: 27.7515 - mean_squared_error: 27.7515 - mean_absolute_error: 2.5960 - root_mean_squared_error: 5.2680 - val_loss: 75.6562 - val_mean_squared_error: 75.6562 - val_mean_absolute_error: 7.6793 - val_root_mean_squared_error: 8.6981\n",
            "Epoch 21/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 27.3559 - mean_squared_error: 27.3558 - mean_absolute_error: 2.5918 - root_mean_squared_error: 5.2303 - val_loss: 898.3641 - val_mean_squared_error: 898.3641 - val_mean_absolute_error: 25.3282 - val_root_mean_squared_error: 29.9727\n",
            "Epoch 22/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 28.0919 - mean_squared_error: 28.0919 - mean_absolute_error: 2.6153 - root_mean_squared_error: 5.3002 - val_loss: 29.6892 - val_mean_squared_error: 29.6892 - val_mean_absolute_error: 2.5215 - val_root_mean_squared_error: 5.4488\n",
            "Epoch 23/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 27.3290 - mean_squared_error: 27.3290 - mean_absolute_error: 2.5645 - root_mean_squared_error: 5.2277 - val_loss: 1158.0302 - val_mean_squared_error: 1158.0302 - val_mean_absolute_error: 28.5468 - val_root_mean_squared_error: 34.0298\n",
            "Epoch 24/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 27.1278 - mean_squared_error: 27.1278 - mean_absolute_error: 2.6020 - root_mean_squared_error: 5.2084 - val_loss: 37.1613 - val_mean_squared_error: 37.1613 - val_mean_absolute_error: 4.4786 - val_root_mean_squared_error: 6.0960\n",
            "Epoch 25/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 28.3660 - mean_squared_error: 28.3660 - mean_absolute_error: 2.6336 - root_mean_squared_error: 5.3260 - val_loss: 493.2627 - val_mean_squared_error: 493.2627 - val_mean_absolute_error: 19.1272 - val_root_mean_squared_error: 22.2095\n",
            "Epoch 26/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 27.7682 - mean_squared_error: 27.7682 - mean_absolute_error: 2.6118 - root_mean_squared_error: 5.2696 - val_loss: 349.4187 - val_mean_squared_error: 349.4187 - val_mean_absolute_error: 16.1581 - val_root_mean_squared_error: 18.6927\n",
            "Epoch 27/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 26.4449 - mean_squared_error: 26.4449 - mean_absolute_error: 2.5302 - root_mean_squared_error: 5.1425 - val_loss: 368.3345 - val_mean_squared_error: 368.3345 - val_mean_absolute_error: 16.5112 - val_root_mean_squared_error: 19.1920\n",
            "Epoch 28/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 26.0962 - mean_squared_error: 26.0962 - mean_absolute_error: 2.5433 - root_mean_squared_error: 5.1084 - val_loss: 29.1425 - val_mean_squared_error: 29.1425 - val_mean_absolute_error: 2.5500 - val_root_mean_squared_error: 5.3984\n",
            "Epoch 29/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 26.1596 - mean_squared_error: 26.1596 - mean_absolute_error: 2.5453 - root_mean_squared_error: 5.1146 - val_loss: 30.2488 - val_mean_squared_error: 30.2488 - val_mean_absolute_error: 2.5490 - val_root_mean_squared_error: 5.4999\n",
            "Epoch 30/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 25.3287 - mean_squared_error: 25.3287 - mean_absolute_error: 2.5065 - root_mean_squared_error: 5.0328 - val_loss: 123.7388 - val_mean_squared_error: 123.7388 - val_mean_absolute_error: 9.6180 - val_root_mean_squared_error: 11.1238\n",
            "Epoch 31/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 25.6240 - mean_squared_error: 25.6240 - mean_absolute_error: 2.5029 - root_mean_squared_error: 5.0620 - val_loss: 32.4161 - val_mean_squared_error: 32.4161 - val_mean_absolute_error: 2.7682 - val_root_mean_squared_error: 5.6935\n",
            "Epoch 32/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 25.3498 - mean_squared_error: 25.3498 - mean_absolute_error: 2.4816 - root_mean_squared_error: 5.0349 - val_loss: 30.8660 - val_mean_squared_error: 30.8660 - val_mean_absolute_error: 2.5739 - val_root_mean_squared_error: 5.5557\n",
            "Epoch 33/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 27.5764 - mean_squared_error: 27.5764 - mean_absolute_error: 2.5997 - root_mean_squared_error: 5.2513 - val_loss: 1299.8179 - val_mean_squared_error: 1299.8179 - val_mean_absolute_error: 29.9019 - val_root_mean_squared_error: 36.0530\n",
            "Epoch 34/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 27.7318 - mean_squared_error: 27.7318 - mean_absolute_error: 2.6015 - root_mean_squared_error: 5.2661 - val_loss: 430.0123 - val_mean_squared_error: 430.0123 - val_mean_absolute_error: 17.6433 - val_root_mean_squared_error: 20.7367\n",
            "Epoch 35/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 28.1043 - mean_squared_error: 28.1043 - mean_absolute_error: 2.6199 - root_mean_squared_error: 5.3014 - val_loss: 807.8254 - val_mean_squared_error: 807.8254 - val_mean_absolute_error: 24.1321 - val_root_mean_squared_error: 28.4223\n",
            "Epoch 36/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 27.1605 - mean_squared_error: 27.1605 - mean_absolute_error: 2.6071 - root_mean_squared_error: 5.2116 - val_loss: 27.7138 - val_mean_squared_error: 27.7138 - val_mean_absolute_error: 2.4525 - val_root_mean_squared_error: 5.2644\n",
            "Epoch 37/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 27.1587 - mean_squared_error: 27.1587 - mean_absolute_error: 2.6142 - root_mean_squared_error: 5.2114 - val_loss: 27.1931 - val_mean_squared_error: 27.1931 - val_mean_absolute_error: 2.4675 - val_root_mean_squared_error: 5.2147\n",
            "Epoch 38/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 26.6151 - mean_squared_error: 26.6151 - mean_absolute_error: 2.5571 - root_mean_squared_error: 5.1590 - val_loss: 30.2102 - val_mean_squared_error: 30.2102 - val_mean_absolute_error: 2.5536 - val_root_mean_squared_error: 5.4964\n",
            "Epoch 39/150\n",
            "47/47 [==============================] - 96s 2s/step - loss: 26.9492 - mean_squared_error: 26.9492 - mean_absolute_error: 2.5875 - root_mean_squared_error: 5.1913 - val_loss: 44.6340 - val_mean_squared_error: 44.6340 - val_mean_absolute_error: 4.6275 - val_root_mean_squared_error: 6.6809\n",
            "Epoch 40/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 26.2054 - mean_squared_error: 26.2054 - mean_absolute_error: 2.5246 - root_mean_squared_error: 5.1191 - val_loss: 30.6150 - val_mean_squared_error: 30.6150 - val_mean_absolute_error: 2.8273 - val_root_mean_squared_error: 5.5331\n",
            "Epoch 41/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 25.9373 - mean_squared_error: 25.9373 - mean_absolute_error: 2.5201 - root_mean_squared_error: 5.0929 - val_loss: 27.5989 - val_mean_squared_error: 27.5989 - val_mean_absolute_error: 2.4333 - val_root_mean_squared_error: 5.2535\n",
            "Epoch 42/150\n",
            "47/47 [==============================] - 96s 2s/step - loss: 28.6086 - mean_squared_error: 28.6086 - mean_absolute_error: 2.6826 - root_mean_squared_error: 5.3487 - val_loss: 34.5714 - val_mean_squared_error: 34.5714 - val_mean_absolute_error: 2.7003 - val_root_mean_squared_error: 5.8797\n",
            "Epoch 43/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 26.9739 - mean_squared_error: 26.9739 - mean_absolute_error: 2.5565 - root_mean_squared_error: 5.1936 - val_loss: 1352.9303 - val_mean_squared_error: 1352.9302 - val_mean_absolute_error: 30.3779 - val_root_mean_squared_error: 36.7822\n",
            "Epoch 44/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 26.4441 - mean_squared_error: 26.4441 - mean_absolute_error: 2.5584 - root_mean_squared_error: 5.1424 - val_loss: 1382.6604 - val_mean_squared_error: 1382.6604 - val_mean_absolute_error: 30.6234 - val_root_mean_squared_error: 37.1841\n",
            "Epoch 45/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 25.6549 - mean_squared_error: 25.6549 - mean_absolute_error: 2.4940 - root_mean_squared_error: 5.0651 - val_loss: 590.0663 - val_mean_squared_error: 590.0662 - val_mean_absolute_error: 20.5815 - val_root_mean_squared_error: 24.2913\n",
            "Epoch 46/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 26.7276 - mean_squared_error: 26.7276 - mean_absolute_error: 2.5927 - root_mean_squared_error: 5.1699 - val_loss: 97.1665 - val_mean_squared_error: 97.1665 - val_mean_absolute_error: 7.8165 - val_root_mean_squared_error: 9.8573\n",
            "Epoch 47/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 25.3123 - mean_squared_error: 25.3123 - mean_absolute_error: 2.5028 - root_mean_squared_error: 5.0311 - val_loss: 33.0201 - val_mean_squared_error: 33.0201 - val_mean_absolute_error: 2.6570 - val_root_mean_squared_error: 5.7463\n",
            "Epoch 48/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 27.4558 - mean_squared_error: 27.4558 - mean_absolute_error: 2.6097 - root_mean_squared_error: 5.2398 - val_loss: 77.0583 - val_mean_squared_error: 77.0583 - val_mean_absolute_error: 7.0996 - val_root_mean_squared_error: 8.7783\n",
            "Epoch 49/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.5353 - mean_squared_error: 24.5353 - mean_absolute_error: 2.4531 - root_mean_squared_error: 4.9533 - val_loss: 29.2770 - val_mean_squared_error: 29.2770 - val_mean_absolute_error: 2.5175 - val_root_mean_squared_error: 5.4108\n",
            "Epoch 50/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.8272 - mean_squared_error: 24.8272 - mean_absolute_error: 2.4690 - root_mean_squared_error: 4.9827 - val_loss: 521.2619 - val_mean_squared_error: 521.2619 - val_mean_absolute_error: 19.3878 - val_root_mean_squared_error: 22.8312\n",
            "Epoch 51/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 25.6235 - mean_squared_error: 25.6235 - mean_absolute_error: 2.4889 - root_mean_squared_error: 5.0620 - val_loss: 614.6732 - val_mean_squared_error: 614.6732 - val_mean_absolute_error: 21.4604 - val_root_mean_squared_error: 24.7926\n",
            "Epoch 52/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 25.8881 - mean_squared_error: 25.8881 - mean_absolute_error: 2.5401 - root_mean_squared_error: 5.0880 - val_loss: 28.4159 - val_mean_squared_error: 28.4159 - val_mean_absolute_error: 2.5173 - val_root_mean_squared_error: 5.3307\n",
            "Epoch 53/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 25.3797 - mean_squared_error: 25.3797 - mean_absolute_error: 2.5153 - root_mean_squared_error: 5.0378 - val_loss: 48.3616 - val_mean_squared_error: 48.3616 - val_mean_absolute_error: 4.9423 - val_root_mean_squared_error: 6.9542\n",
            "Epoch 54/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 27.4128 - mean_squared_error: 27.4128 - mean_absolute_error: 2.5911 - root_mean_squared_error: 5.2357 - val_loss: 35.5844 - val_mean_squared_error: 35.5844 - val_mean_absolute_error: 2.7615 - val_root_mean_squared_error: 5.9653\n",
            "Epoch 55/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 24.6775 - mean_squared_error: 24.6775 - mean_absolute_error: 2.4526 - root_mean_squared_error: 4.9676 - val_loss: 29.7749 - val_mean_squared_error: 29.7749 - val_mean_absolute_error: 2.5753 - val_root_mean_squared_error: 5.4566\n",
            "Epoch 56/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 24.6048 - mean_squared_error: 24.6048 - mean_absolute_error: 2.4588 - root_mean_squared_error: 4.9603 - val_loss: 102.5469 - val_mean_squared_error: 102.5469 - val_mean_absolute_error: 8.9648 - val_root_mean_squared_error: 10.1265\n",
            "Epoch 57/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 23.9121 - mean_squared_error: 23.9121 - mean_absolute_error: 2.3981 - root_mean_squared_error: 4.8900 - val_loss: 32.4197 - val_mean_squared_error: 32.4197 - val_mean_absolute_error: 2.7655 - val_root_mean_squared_error: 5.6938\n",
            "Epoch 58/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 25.0545 - mean_squared_error: 25.0545 - mean_absolute_error: 2.5018 - root_mean_squared_error: 5.0054 - val_loss: 31.2375 - val_mean_squared_error: 31.2375 - val_mean_absolute_error: 2.5802 - val_root_mean_squared_error: 5.5891\n",
            "Epoch 59/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 23.5697 - mean_squared_error: 23.5697 - mean_absolute_error: 2.4200 - root_mean_squared_error: 4.8549 - val_loss: 29.3475 - val_mean_squared_error: 29.3475 - val_mean_absolute_error: 2.5145 - val_root_mean_squared_error: 5.4173\n",
            "Epoch 60/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 24.6358 - mean_squared_error: 24.6358 - mean_absolute_error: 2.4720 - root_mean_squared_error: 4.9634 - val_loss: 51.9485 - val_mean_squared_error: 51.9485 - val_mean_absolute_error: 4.9390 - val_root_mean_squared_error: 7.2075\n",
            "Epoch 61/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 25.2554 - mean_squared_error: 25.2554 - mean_absolute_error: 2.4873 - root_mean_squared_error: 5.0255 - val_loss: 37.2066 - val_mean_squared_error: 37.2066 - val_mean_absolute_error: 3.6832 - val_root_mean_squared_error: 6.0997\n",
            "Epoch 62/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 24.7782 - mean_squared_error: 24.7782 - mean_absolute_error: 2.4576 - root_mean_squared_error: 4.9778 - val_loss: 38.7029 - val_mean_squared_error: 38.7029 - val_mean_absolute_error: 3.8589 - val_root_mean_squared_error: 6.2212\n",
            "Epoch 63/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 25.3145 - mean_squared_error: 25.3145 - mean_absolute_error: 2.5219 - root_mean_squared_error: 5.0313 - val_loss: 152.1749 - val_mean_squared_error: 152.1749 - val_mean_absolute_error: 10.0232 - val_root_mean_squared_error: 12.3359\n",
            "Epoch 64/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 24.9903 - mean_squared_error: 24.9903 - mean_absolute_error: 2.4671 - root_mean_squared_error: 4.9990 - val_loss: 235.5687 - val_mean_squared_error: 235.5687 - val_mean_absolute_error: 12.9877 - val_root_mean_squared_error: 15.3482\n",
            "Epoch 65/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 26.1059 - mean_squared_error: 26.1059 - mean_absolute_error: 2.5285 - root_mean_squared_error: 5.1094 - val_loss: 102.9199 - val_mean_squared_error: 102.9199 - val_mean_absolute_error: 8.1285 - val_root_mean_squared_error: 10.1449\n",
            "Epoch 66/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 23.6834 - mean_squared_error: 23.6833 - mean_absolute_error: 2.4113 - root_mean_squared_error: 4.8666 - val_loss: 46.3830 - val_mean_squared_error: 46.3830 - val_mean_absolute_error: 4.6278 - val_root_mean_squared_error: 6.8105\n",
            "Epoch 67/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 25.5419 - mean_squared_error: 25.5419 - mean_absolute_error: 2.5134 - root_mean_squared_error: 5.0539 - val_loss: 34.8127 - val_mean_squared_error: 34.8127 - val_mean_absolute_error: 3.1770 - val_root_mean_squared_error: 5.9002\n",
            "Epoch 68/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 24.5334 - mean_squared_error: 24.5334 - mean_absolute_error: 2.4482 - root_mean_squared_error: 4.9531 - val_loss: 29.3396 - val_mean_squared_error: 29.3396 - val_mean_absolute_error: 2.5274 - val_root_mean_squared_error: 5.4166\n",
            "Epoch 69/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 25.0995 - mean_squared_error: 25.0995 - mean_absolute_error: 2.4994 - root_mean_squared_error: 5.0099 - val_loss: 63.1635 - val_mean_squared_error: 63.1635 - val_mean_absolute_error: 5.7797 - val_root_mean_squared_error: 7.9475\n",
            "Epoch 70/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.9641 - mean_squared_error: 22.9641 - mean_absolute_error: 2.3622 - root_mean_squared_error: 4.7921 - val_loss: 28.3031 - val_mean_squared_error: 28.3031 - val_mean_absolute_error: 2.4990 - val_root_mean_squared_error: 5.3201\n",
            "Epoch 71/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 24.3423 - mean_squared_error: 24.3423 - mean_absolute_error: 2.4471 - root_mean_squared_error: 4.9338 - val_loss: 28.3371 - val_mean_squared_error: 28.3371 - val_mean_absolute_error: 2.4726 - val_root_mean_squared_error: 5.3233\n",
            "Epoch 72/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 23.0592 - mean_squared_error: 23.0592 - mean_absolute_error: 2.3897 - root_mean_squared_error: 4.8020 - val_loss: 30.1127 - val_mean_squared_error: 30.1127 - val_mean_absolute_error: 2.5685 - val_root_mean_squared_error: 5.4875\n",
            "Epoch 73/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 23.6095 - mean_squared_error: 23.6095 - mean_absolute_error: 2.4105 - root_mean_squared_error: 4.8590 - val_loss: 217.3517 - val_mean_squared_error: 217.3517 - val_mean_absolute_error: 11.5400 - val_root_mean_squared_error: 14.7429\n",
            "Epoch 74/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 23.6050 - mean_squared_error: 23.6050 - mean_absolute_error: 2.4075 - root_mean_squared_error: 4.8585 - val_loss: 421.6899 - val_mean_squared_error: 421.6899 - val_mean_absolute_error: 17.6814 - val_root_mean_squared_error: 20.5351\n",
            "Epoch 75/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 23.1753 - mean_squared_error: 23.1753 - mean_absolute_error: 2.3709 - root_mean_squared_error: 4.8141 - val_loss: 219.9532 - val_mean_squared_error: 219.9532 - val_mean_absolute_error: 12.7231 - val_root_mean_squared_error: 14.8308\n",
            "Epoch 76/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 23.2448 - mean_squared_error: 23.2448 - mean_absolute_error: 2.3839 - root_mean_squared_error: 4.8213 - val_loss: 30.8592 - val_mean_squared_error: 30.8592 - val_mean_absolute_error: 2.6259 - val_root_mean_squared_error: 5.5551\n",
            "Epoch 77/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 23.7306 - mean_squared_error: 23.7306 - mean_absolute_error: 2.4214 - root_mean_squared_error: 4.8714 - val_loss: 323.9837 - val_mean_squared_error: 323.9837 - val_mean_absolute_error: 14.3729 - val_root_mean_squared_error: 17.9995\n",
            "Epoch 78/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 24.2976 - mean_squared_error: 24.2976 - mean_absolute_error: 2.4555 - root_mean_squared_error: 4.9293 - val_loss: 36.2483 - val_mean_squared_error: 36.2482 - val_mean_absolute_error: 2.8071 - val_root_mean_squared_error: 6.0207\n",
            "Epoch 79/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 25.6015 - mean_squared_error: 25.6015 - mean_absolute_error: 2.4643 - root_mean_squared_error: 5.0598 - val_loss: 441.4598 - val_mean_squared_error: 441.4598 - val_mean_absolute_error: 17.2973 - val_root_mean_squared_error: 21.0109\n",
            "Epoch 80/150\n",
            "47/47 [==============================] - 103s 2s/step - loss: 24.0425 - mean_squared_error: 24.0425 - mean_absolute_error: 2.4528 - root_mean_squared_error: 4.9033 - val_loss: 78.5070 - val_mean_squared_error: 78.5070 - val_mean_absolute_error: 6.6841 - val_root_mean_squared_error: 8.8604\n",
            "Epoch 81/150\n",
            "47/47 [==============================] - 103s 2s/step - loss: 25.7608 - mean_squared_error: 25.7608 - mean_absolute_error: 2.4968 - root_mean_squared_error: 5.0755 - val_loss: 30.4722 - val_mean_squared_error: 30.4722 - val_mean_absolute_error: 2.6083 - val_root_mean_squared_error: 5.5202\n",
            "Epoch 82/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 25.6686 - mean_squared_error: 25.6686 - mean_absolute_error: 2.5279 - root_mean_squared_error: 5.0664 - val_loss: 30.6167 - val_mean_squared_error: 30.6167 - val_mean_absolute_error: 2.5406 - val_root_mean_squared_error: 5.5332\n",
            "Epoch 83/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 23.9740 - mean_squared_error: 23.9740 - mean_absolute_error: 2.4150 - root_mean_squared_error: 4.8963 - val_loss: 34.3873 - val_mean_squared_error: 34.3873 - val_mean_absolute_error: 2.7371 - val_root_mean_squared_error: 5.8641\n",
            "Epoch 84/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 24.8270 - mean_squared_error: 24.8270 - mean_absolute_error: 2.4763 - root_mean_squared_error: 4.9827 - val_loss: 31.1949 - val_mean_squared_error: 31.1949 - val_mean_absolute_error: 2.6384 - val_root_mean_squared_error: 5.5852\n",
            "Epoch 85/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 26.3105 - mean_squared_error: 26.3105 - mean_absolute_error: 2.5533 - root_mean_squared_error: 5.1294 - val_loss: 28.6975 - val_mean_squared_error: 28.6975 - val_mean_absolute_error: 2.5068 - val_root_mean_squared_error: 5.3570\n",
            "Epoch 86/150\n",
            "47/47 [==============================] - 102s 2s/step - loss: 24.0104 - mean_squared_error: 24.0104 - mean_absolute_error: 2.4118 - root_mean_squared_error: 4.9000 - val_loss: 31.9984 - val_mean_squared_error: 31.9984 - val_mean_absolute_error: 2.6297 - val_root_mean_squared_error: 5.6567\n",
            "Epoch 87/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.0974 - mean_squared_error: 24.0974 - mean_absolute_error: 2.4380 - root_mean_squared_error: 4.9089 - val_loss: 31.9708 - val_mean_squared_error: 31.9708 - val_mean_absolute_error: 2.6033 - val_root_mean_squared_error: 5.6543\n",
            "Epoch 88/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 23.9639 - mean_squared_error: 23.9639 - mean_absolute_error: 2.4371 - root_mean_squared_error: 4.8953 - val_loss: 37.0441 - val_mean_squared_error: 37.0441 - val_mean_absolute_error: 2.8222 - val_root_mean_squared_error: 6.0864\n",
            "Epoch 89/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.5000 - mean_squared_error: 24.5000 - mean_absolute_error: 2.4759 - root_mean_squared_error: 4.9497 - val_loss: 33.0372 - val_mean_squared_error: 33.0372 - val_mean_absolute_error: 2.6272 - val_root_mean_squared_error: 5.7478\n",
            "Epoch 90/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 23.4211 - mean_squared_error: 23.4212 - mean_absolute_error: 2.4098 - root_mean_squared_error: 4.8395 - val_loss: 340.4710 - val_mean_squared_error: 340.4710 - val_mean_absolute_error: 14.7513 - val_root_mean_squared_error: 18.4519\n",
            "Epoch 91/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.0420 - mean_squared_error: 24.0420 - mean_absolute_error: 2.4362 - root_mean_squared_error: 4.9033 - val_loss: 390.2556 - val_mean_squared_error: 390.2556 - val_mean_absolute_error: 16.4578 - val_root_mean_squared_error: 19.7549\n",
            "Epoch 92/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 22.8444 - mean_squared_error: 22.8444 - mean_absolute_error: 2.3639 - root_mean_squared_error: 4.7796 - val_loss: 210.9548 - val_mean_squared_error: 210.9548 - val_mean_absolute_error: 10.7513 - val_root_mean_squared_error: 14.5243\n",
            "Epoch 93/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 25.7211 - mean_squared_error: 25.7211 - mean_absolute_error: 2.5196 - root_mean_squared_error: 5.0716 - val_loss: 29.8328 - val_mean_squared_error: 29.8328 - val_mean_absolute_error: 2.5840 - val_root_mean_squared_error: 5.4619\n",
            "Epoch 94/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.2198 - mean_squared_error: 22.2198 - mean_absolute_error: 2.3359 - root_mean_squared_error: 4.7138 - val_loss: 37.5713 - val_mean_squared_error: 37.5713 - val_mean_absolute_error: 3.6488 - val_root_mean_squared_error: 6.1295\n",
            "Epoch 95/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 24.7721 - mean_squared_error: 24.7721 - mean_absolute_error: 2.4592 - root_mean_squared_error: 4.9772 - val_loss: 66.5379 - val_mean_squared_error: 66.5379 - val_mean_absolute_error: 5.9363 - val_root_mean_squared_error: 8.1571\n",
            "Epoch 96/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 23.7133 - mean_squared_error: 23.7133 - mean_absolute_error: 2.4161 - root_mean_squared_error: 4.8696 - val_loss: 29.7064 - val_mean_squared_error: 29.7064 - val_mean_absolute_error: 2.5213 - val_root_mean_squared_error: 5.4504\n",
            "Epoch 97/150\n",
            "47/47 [==============================] - 96s 2s/step - loss: 23.9393 - mean_squared_error: 23.9393 - mean_absolute_error: 2.4331 - root_mean_squared_error: 4.8928 - val_loss: 30.5936 - val_mean_squared_error: 30.5936 - val_mean_absolute_error: 2.5588 - val_root_mean_squared_error: 5.5311\n",
            "Epoch 98/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.3962 - mean_squared_error: 24.3962 - mean_absolute_error: 2.4632 - root_mean_squared_error: 4.9392 - val_loss: 28.7998 - val_mean_squared_error: 28.7998 - val_mean_absolute_error: 2.5091 - val_root_mean_squared_error: 5.3665\n",
            "Epoch 99/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 23.6260 - mean_squared_error: 23.6260 - mean_absolute_error: 2.4021 - root_mean_squared_error: 4.8607 - val_loss: 38.8154 - val_mean_squared_error: 38.8154 - val_mean_absolute_error: 3.9977 - val_root_mean_squared_error: 6.2302\n",
            "Epoch 100/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 23.1045 - mean_squared_error: 23.1045 - mean_absolute_error: 2.3888 - root_mean_squared_error: 4.8067 - val_loss: 152.4659 - val_mean_squared_error: 152.4660 - val_mean_absolute_error: 10.1210 - val_root_mean_squared_error: 12.3477\n",
            "Epoch 101/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.5833 - mean_squared_error: 22.5833 - mean_absolute_error: 2.3677 - root_mean_squared_error: 4.7522 - val_loss: 235.9157 - val_mean_squared_error: 235.9157 - val_mean_absolute_error: 12.6995 - val_root_mean_squared_error: 15.3595\n",
            "Epoch 102/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 22.8990 - mean_squared_error: 22.8990 - mean_absolute_error: 2.3868 - root_mean_squared_error: 4.7853 - val_loss: 394.8700 - val_mean_squared_error: 394.8700 - val_mean_absolute_error: 16.1756 - val_root_mean_squared_error: 19.8713\n",
            "Epoch 103/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.0601 - mean_squared_error: 22.0601 - mean_absolute_error: 2.3355 - root_mean_squared_error: 4.6968 - val_loss: 30.3639 - val_mean_squared_error: 30.3639 - val_mean_absolute_error: 2.5966 - val_root_mean_squared_error: 5.5103\n",
            "Epoch 104/150\n",
            "47/47 [==============================] - 95s 2s/step - loss: 22.2693 - mean_squared_error: 22.2693 - mean_absolute_error: 2.3300 - root_mean_squared_error: 4.7190 - val_loss: 35.9380 - val_mean_squared_error: 35.9380 - val_mean_absolute_error: 3.3299 - val_root_mean_squared_error: 5.9948\n",
            "Epoch 105/150\n",
            "47/47 [==============================] - 95s 2s/step - loss: 22.3786 - mean_squared_error: 22.3786 - mean_absolute_error: 2.3724 - root_mean_squared_error: 4.7306 - val_loss: 92.1970 - val_mean_squared_error: 92.1970 - val_mean_absolute_error: 6.7688 - val_root_mean_squared_error: 9.6019\n",
            "Epoch 106/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 24.7720 - mean_squared_error: 24.7720 - mean_absolute_error: 2.4721 - root_mean_squared_error: 4.9771 - val_loss: 30.2064 - val_mean_squared_error: 30.2064 - val_mean_absolute_error: 2.5855 - val_root_mean_squared_error: 5.4960\n",
            "Epoch 107/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 22.0551 - mean_squared_error: 22.0552 - mean_absolute_error: 2.3314 - root_mean_squared_error: 4.6963 - val_loss: 30.9114 - val_mean_squared_error: 30.9114 - val_mean_absolute_error: 2.6669 - val_root_mean_squared_error: 5.5598\n",
            "Epoch 108/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.7135 - mean_squared_error: 22.7135 - mean_absolute_error: 2.3493 - root_mean_squared_error: 4.7659 - val_loss: 29.2373 - val_mean_squared_error: 29.2373 - val_mean_absolute_error: 2.5723 - val_root_mean_squared_error: 5.4072\n",
            "Epoch 109/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.7047 - mean_squared_error: 22.7047 - mean_absolute_error: 2.3552 - root_mean_squared_error: 4.7649 - val_loss: 31.7524 - val_mean_squared_error: 31.7524 - val_mean_absolute_error: 2.6207 - val_root_mean_squared_error: 5.6349\n",
            "Epoch 110/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 21.7947 - mean_squared_error: 21.7947 - mean_absolute_error: 2.3160 - root_mean_squared_error: 4.6685 - val_loss: 53.6682 - val_mean_squared_error: 53.6683 - val_mean_absolute_error: 4.7450 - val_root_mean_squared_error: 7.3259\n",
            "Epoch 111/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 23.0779 - mean_squared_error: 23.0779 - mean_absolute_error: 2.3705 - root_mean_squared_error: 4.8039 - val_loss: 680.0547 - val_mean_squared_error: 680.0546 - val_mean_absolute_error: 20.9234 - val_root_mean_squared_error: 26.0779\n",
            "Epoch 112/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.0243 - mean_squared_error: 22.0243 - mean_absolute_error: 2.3517 - root_mean_squared_error: 4.6930 - val_loss: 30.2868 - val_mean_squared_error: 30.2868 - val_mean_absolute_error: 2.5549 - val_root_mean_squared_error: 5.5033\n",
            "Epoch 113/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 23.1256 - mean_squared_error: 23.1256 - mean_absolute_error: 2.3768 - root_mean_squared_error: 4.8089 - val_loss: 260.0025 - val_mean_squared_error: 260.0025 - val_mean_absolute_error: 13.9010 - val_root_mean_squared_error: 16.1246\n",
            "Epoch 114/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 22.3950 - mean_squared_error: 22.3950 - mean_absolute_error: 2.3561 - root_mean_squared_error: 4.7323 - val_loss: 69.6299 - val_mean_squared_error: 69.6299 - val_mean_absolute_error: 5.7425 - val_root_mean_squared_error: 8.3445\n",
            "Epoch 115/150\n",
            "47/47 [==============================] - 96s 2s/step - loss: 22.8701 - mean_squared_error: 22.8701 - mean_absolute_error: 2.3535 - root_mean_squared_error: 4.7823 - val_loss: 30.5084 - val_mean_squared_error: 30.5084 - val_mean_absolute_error: 2.6646 - val_root_mean_squared_error: 5.5234\n",
            "Epoch 116/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 21.9367 - mean_squared_error: 21.9367 - mean_absolute_error: 2.3186 - root_mean_squared_error: 4.6837 - val_loss: 28.7804 - val_mean_squared_error: 28.7804 - val_mean_absolute_error: 2.5554 - val_root_mean_squared_error: 5.3647\n",
            "Epoch 117/150\n",
            "47/47 [==============================] - 101s 2s/step - loss: 22.8322 - mean_squared_error: 22.8322 - mean_absolute_error: 2.3471 - root_mean_squared_error: 4.7783 - val_loss: 156.9258 - val_mean_squared_error: 156.9258 - val_mean_absolute_error: 9.4953 - val_root_mean_squared_error: 12.5270\n",
            "Epoch 118/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 23.6389 - mean_squared_error: 23.6389 - mean_absolute_error: 2.3797 - root_mean_squared_error: 4.8620 - val_loss: 31.8139 - val_mean_squared_error: 31.8139 - val_mean_absolute_error: 2.6238 - val_root_mean_squared_error: 5.6404\n",
            "Epoch 119/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 25.9604 - mean_squared_error: 25.9604 - mean_absolute_error: 2.5395 - root_mean_squared_error: 5.0951 - val_loss: 1284.5813 - val_mean_squared_error: 1284.5814 - val_mean_absolute_error: 29.6189 - val_root_mean_squared_error: 35.8411\n",
            "Epoch 120/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 23.3402 - mean_squared_error: 23.3402 - mean_absolute_error: 2.3918 - root_mean_squared_error: 4.8312 - val_loss: 264.5149 - val_mean_squared_error: 264.5148 - val_mean_absolute_error: 12.9833 - val_root_mean_squared_error: 16.2639\n",
            "Epoch 121/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 23.9745 - mean_squared_error: 23.9745 - mean_absolute_error: 2.4044 - root_mean_squared_error: 4.8964 - val_loss: 220.3654 - val_mean_squared_error: 220.3654 - val_mean_absolute_error: 12.9654 - val_root_mean_squared_error: 14.8447\n",
            "Epoch 122/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 23.2661 - mean_squared_error: 23.2661 - mean_absolute_error: 2.3724 - root_mean_squared_error: 4.8235 - val_loss: 93.7943 - val_mean_squared_error: 93.7943 - val_mean_absolute_error: 6.6470 - val_root_mean_squared_error: 9.6847\n",
            "Epoch 123/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 24.4813 - mean_squared_error: 24.4813 - mean_absolute_error: 2.4743 - root_mean_squared_error: 4.9479 - val_loss: 30.2042 - val_mean_squared_error: 30.2042 - val_mean_absolute_error: 2.5415 - val_root_mean_squared_error: 5.4958\n",
            "Epoch 124/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.9250 - mean_squared_error: 22.9250 - mean_absolute_error: 2.3512 - root_mean_squared_error: 4.7880 - val_loss: 111.4470 - val_mean_squared_error: 111.4470 - val_mean_absolute_error: 7.6313 - val_root_mean_squared_error: 10.5568\n",
            "Epoch 125/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 23.5277 - mean_squared_error: 23.5277 - mean_absolute_error: 2.4049 - root_mean_squared_error: 4.8505 - val_loss: 39.5298 - val_mean_squared_error: 39.5298 - val_mean_absolute_error: 2.8341 - val_root_mean_squared_error: 6.2873\n",
            "Epoch 126/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 24.0876 - mean_squared_error: 24.0876 - mean_absolute_error: 2.4263 - root_mean_squared_error: 4.9079 - val_loss: 142.5470 - val_mean_squared_error: 142.5470 - val_mean_absolute_error: 8.7121 - val_root_mean_squared_error: 11.9393\n",
            "Epoch 127/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 24.0122 - mean_squared_error: 24.0122 - mean_absolute_error: 2.4206 - root_mean_squared_error: 4.9002 - val_loss: 31.6498 - val_mean_squared_error: 31.6498 - val_mean_absolute_error: 2.6169 - val_root_mean_squared_error: 5.6258\n",
            "Epoch 128/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 23.6949 - mean_squared_error: 23.6949 - mean_absolute_error: 2.4133 - root_mean_squared_error: 4.8677 - val_loss: 29.0753 - val_mean_squared_error: 29.0753 - val_mean_absolute_error: 2.5028 - val_root_mean_squared_error: 5.3921\n",
            "Epoch 129/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 22.7165 - mean_squared_error: 22.7165 - mean_absolute_error: 2.3412 - root_mean_squared_error: 4.7662 - val_loss: 30.6082 - val_mean_squared_error: 30.6082 - val_mean_absolute_error: 2.5999 - val_root_mean_squared_error: 5.5325\n",
            "Epoch 130/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.4207 - mean_squared_error: 22.4207 - mean_absolute_error: 2.3493 - root_mean_squared_error: 4.7350 - val_loss: 29.2288 - val_mean_squared_error: 29.2288 - val_mean_absolute_error: 2.5787 - val_root_mean_squared_error: 5.4064\n",
            "Epoch 131/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 21.4779 - mean_squared_error: 21.4779 - mean_absolute_error: 2.2921 - root_mean_squared_error: 4.6344 - val_loss: 31.7005 - val_mean_squared_error: 31.7005 - val_mean_absolute_error: 2.6363 - val_root_mean_squared_error: 5.6303\n",
            "Epoch 132/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 23.0655 - mean_squared_error: 23.0656 - mean_absolute_error: 2.3679 - root_mean_squared_error: 4.8027 - val_loss: 58.3521 - val_mean_squared_error: 58.3521 - val_mean_absolute_error: 4.5372 - val_root_mean_squared_error: 7.6389\n",
            "Epoch 133/150\n",
            "47/47 [==============================] - 97s 2s/step - loss: 21.7495 - mean_squared_error: 21.7495 - mean_absolute_error: 2.3132 - root_mean_squared_error: 4.6636 - val_loss: 33.0011 - val_mean_squared_error: 33.0011 - val_mean_absolute_error: 2.7116 - val_root_mean_squared_error: 5.7447\n",
            "Epoch 134/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.4585 - mean_squared_error: 22.4585 - mean_absolute_error: 2.3659 - root_mean_squared_error: 4.7390 - val_loss: 38.9745 - val_mean_squared_error: 38.9745 - val_mean_absolute_error: 3.7145 - val_root_mean_squared_error: 6.2430\n",
            "Epoch 135/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 21.7029 - mean_squared_error: 21.7029 - mean_absolute_error: 2.3177 - root_mean_squared_error: 4.6586 - val_loss: 29.6639 - val_mean_squared_error: 29.6639 - val_mean_absolute_error: 2.5611 - val_root_mean_squared_error: 5.4465\n",
            "Epoch 136/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 22.9156 - mean_squared_error: 22.9156 - mean_absolute_error: 2.3551 - root_mean_squared_error: 4.7870 - val_loss: 28.2148 - val_mean_squared_error: 28.2148 - val_mean_absolute_error: 2.5176 - val_root_mean_squared_error: 5.3118\n",
            "Epoch 137/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 22.5458 - mean_squared_error: 22.5458 - mean_absolute_error: 2.3671 - root_mean_squared_error: 4.7482 - val_loss: 29.3832 - val_mean_squared_error: 29.3832 - val_mean_absolute_error: 2.5183 - val_root_mean_squared_error: 5.4206\n",
            "Epoch 138/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.5044 - mean_squared_error: 22.5044 - mean_absolute_error: 2.3321 - root_mean_squared_error: 4.7439 - val_loss: 30.8142 - val_mean_squared_error: 30.8142 - val_mean_absolute_error: 2.6658 - val_root_mean_squared_error: 5.5511\n",
            "Epoch 139/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.2825 - mean_squared_error: 22.2825 - mean_absolute_error: 2.3558 - root_mean_squared_error: 4.7204 - val_loss: 29.8579 - val_mean_squared_error: 29.8579 - val_mean_absolute_error: 2.5431 - val_root_mean_squared_error: 5.4642\n",
            "Epoch 140/150\n",
            "47/47 [==============================] - 100s 2s/step - loss: 21.6243 - mean_squared_error: 21.6243 - mean_absolute_error: 2.3198 - root_mean_squared_error: 4.6502 - val_loss: 29.9849 - val_mean_squared_error: 29.9848 - val_mean_absolute_error: 2.5403 - val_root_mean_squared_error: 5.4758\n",
            "Epoch 141/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 21.3821 - mean_squared_error: 21.3821 - mean_absolute_error: 2.3021 - root_mean_squared_error: 4.6241 - val_loss: 35.2292 - val_mean_squared_error: 35.2292 - val_mean_absolute_error: 2.7210 - val_root_mean_squared_error: 5.9354\n",
            "Epoch 142/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 21.9877 - mean_squared_error: 21.9877 - mean_absolute_error: 2.3309 - root_mean_squared_error: 4.6891 - val_loss: 30.7595 - val_mean_squared_error: 30.7595 - val_mean_absolute_error: 2.5899 - val_root_mean_squared_error: 5.5461\n",
            "Epoch 143/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.4100 - mean_squared_error: 22.4100 - mean_absolute_error: 2.3717 - root_mean_squared_error: 4.7339 - val_loss: 38.9780 - val_mean_squared_error: 38.9780 - val_mean_absolute_error: 3.8070 - val_root_mean_squared_error: 6.2432\n",
            "Epoch 144/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 22.4860 - mean_squared_error: 22.4860 - mean_absolute_error: 2.3502 - root_mean_squared_error: 4.7419 - val_loss: 28.8588 - val_mean_squared_error: 28.8588 - val_mean_absolute_error: 2.5025 - val_root_mean_squared_error: 5.3720\n",
            "Epoch 145/150\n",
            "47/47 [==============================] - 98s 2s/step - loss: 24.9182 - mean_squared_error: 24.9182 - mean_absolute_error: 2.5035 - root_mean_squared_error: 4.9918 - val_loss: 119.7972 - val_mean_squared_error: 119.7972 - val_mean_absolute_error: 9.0679 - val_root_mean_squared_error: 10.9452\n",
            "Epoch 146/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 23.3886 - mean_squared_error: 23.3886 - mean_absolute_error: 2.3935 - root_mean_squared_error: 4.8362 - val_loss: 31.2561 - val_mean_squared_error: 31.2561 - val_mean_absolute_error: 2.6264 - val_root_mean_squared_error: 5.5907\n",
            "Epoch 147/150\n",
            "47/47 [==============================] - 96s 2s/step - loss: 24.0731 - mean_squared_error: 24.0731 - mean_absolute_error: 2.4449 - root_mean_squared_error: 4.9064 - val_loss: 29.4082 - val_mean_squared_error: 29.4082 - val_mean_absolute_error: 2.5262 - val_root_mean_squared_error: 5.4229\n",
            "Epoch 148/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.7920 - mean_squared_error: 22.7920 - mean_absolute_error: 2.3790 - root_mean_squared_error: 4.7741 - val_loss: 34.2629 - val_mean_squared_error: 34.2629 - val_mean_absolute_error: 2.7698 - val_root_mean_squared_error: 5.8534\n",
            "Epoch 149/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.2548 - mean_squared_error: 22.2548 - mean_absolute_error: 2.3162 - root_mean_squared_error: 4.7175 - val_loss: 49.3823 - val_mean_squared_error: 49.3823 - val_mean_absolute_error: 5.0992 - val_root_mean_squared_error: 7.0273\n",
            "Epoch 150/150\n",
            "47/47 [==============================] - 99s 2s/step - loss: 22.4846 - mean_squared_error: 22.4846 - mean_absolute_error: 2.3397 - root_mean_squared_error: 4.7418 - val_loss: 31.1366 - val_mean_squared_error: 31.1366 - val_mean_absolute_error: 2.6018 - val_root_mean_squared_error: 5.5800\n"
          ]
        }
      ],
      "source": [
        "# Rolling mean on BIS data\n",
        "y = pd.DataFrame(bis.train_dataset[:,:,0].T).rolling(min_periods=1,window=3, center=True).mean().to_numpy().T[:,:,np.newaxis]\n",
        "\n",
        "class SaveModelHistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_interval, model_path, history_path):\n",
        "        super(SaveModelHistoryCallback, self).__init__()\n",
        "        self.save_interval = save_interval\n",
        "        self.model_path = model_path\n",
        "        self.history_path = history_path\n",
        "        self.history = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.save_interval == 0:\n",
        "            # Save model\n",
        "            self.model.save(self.model_path.format(epoch=epoch+1))\n",
        "            # Save history\n",
        "            self.history.append(logs)\n",
        "            with open(self.history_path.format(epoch=epoch+1), 'wb') as f:\n",
        "                pickle.dump(self.history, f)\n",
        "\n",
        "save_interval = 20\n",
        "model_path = 'model_epoch_{epoch}.h5'\n",
        "history_path = 'history_epoch_{epoch}.pkl'\n",
        "save_model_history_callback = SaveModelHistoryCallback(save_interval, model_path, history_path)\n",
        "\n",
        "\n",
        "\n",
        "if in_google_colab():\n",
        "    # Train the model\n",
        "    history = model.fit([bloodpressure.train_dataset, etCO2.train_dataset, spO2.train_dataset, mac.train_dataset, info.train_dataset],\n",
        "                        y,\n",
        "                        validation_data=([bloodpressure.validation_dataset, etCO2.validation_dataset, spO2.validation_dataset, mac.validation_dataset, info.validation_dataset], bis.validation_dataset),\n",
        "                        epochs=150,\n",
        "                        callbacks=[save_model_history_callback],\n",
        "                        batch_size=4\n",
        "                        )\n",
        "\n",
        "    train_score = history.history\n",
        "\n",
        "    # Save the model\n",
        "    model.save('download/model.keras')\n",
        "\n",
        "    # Save the training history\n",
        "    with open('download/train_score.pkl', 'wb') as f:\n",
        "        pickle.dump(train_score, f)\n",
        "\n",
        "    # Save the prediction\n",
        "    y_pred = model.predict([bloodpressure.test_dataset, etCO2.test_dataset, spO2.test_dataset, mac.test_dataset, info.test_dataset], verbose=0)\n",
        "    with open('download/prediction.pkl', 'wb') as f:\n",
        "        pickle.dump(y_pred, f)\n",
        "\n",
        "else:\n",
        "    # Load train score data\n",
        "    with open('train_score.pkl', 'rb') as f:\n",
        "        train_score = pickle.load(f)\n",
        "\n",
        "    # Load test prediction data\n",
        "    with open('prediction.pkl', 'rb') as f:\n",
        "        y_pred = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK8Y_JnX1gg8"
      },
      "source": [
        "# Training results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx2x5QBz1gg9"
      },
      "outputs": [],
      "source": [
        "from utils.plotting import training_loss_plot\n",
        "\n",
        "plot = training_loss_plot(train_score, filename='download/training_loss.pdf')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgIiyrCx1gg9"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08ZAoAV41gg-"
      },
      "outputs": [],
      "source": [
        "### Predict on the test set\n",
        "from utils.evaluation import phases_report, phases_report_std\n",
        "\n",
        "print('Testmetriken:')\n",
        "\n",
        "report = phases_report(y_pred, bis.test_dataset, mac.test_dataset)\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w9qgaHj1gg-"
      },
      "outputs": [],
      "source": [
        "phases_report_std(report, y_pred, bis.test_dataset, mac.test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmTsCM271gg-"
      },
      "outputs": [],
      "source": [
        "from utils.plotting import full_histogramm_plot\n",
        "\n",
        "plot = full_histogramm_plot(groundtruth = bis.test_dataset, prediction = y_pred, filename='download/histogramm.pdf')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH48rOxA1gg_"
      },
      "outputs": [],
      "source": [
        "from utils.plotting import single_prediction_plot\n",
        "\n",
        "for case in test_index:\n",
        "    single_prediction_plot(\n",
        "        case = case,\n",
        "        index = test_index,\n",
        "        groundtruth = bis.test_dataset,\n",
        "        prediction = y_pred,\n",
        "        infusion = mac.test_dataset,\n",
        "        error = 'Prediction RMSE',\n",
        "        filename = 'download/' + str(case) + '.pdf')\n",
        "\n",
        "print('Finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzU-hUmI1gg_"
      },
      "outputs": [],
      "source": [
        "from utils.plotting import full_prediction_plot\n",
        "\n",
        "full_prediction_plot(index = test_index, groundtruth = bis.test_dataset, prediction = y_pred, infusion = mac.test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}